from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from prophet import Prophet
import logging
from google.cloud import bigquery
import os
import json

# [í™˜ê²½ ì„¤ì •]
PROJECT_ID = os.getenv("VERTEX_AI_PROJECT_ID", "project-5b75bb04-485d-454e-af7")
DATASET_ID = "tilda"
PRICE_TABLE = "corn_price"
TARGET_TABLE = "prophet_corn"
COMMODITY = "corn"
START_DATE = datetime(2024, 1, 1)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': START_DATE,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def get_bq_client():
    return bigquery.Client(project=PROJECT_ID)

def generate_features(**context):
    """
    Airflow ì‹¤í–‰ ë‚ ì§œ(ds)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Prophet ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    target_date_str = context['ds']
    target_date = pd.to_datetime(target_date_str)
    
    logging.info(f"[PRICE-MODEL][{COMMODITY}] ðŸš€ Task ì‹œìž‘: generate_features (Target: {target_date_str})")
    
    client = get_bq_client()
    
    # 1. í•™ìŠµìš© ê³¼ê±° ë°ì´í„° ì¡°íšŒ (íƒ€ê²Ÿ ë‚ ì§œ ê¸°ì¤€ ê³¼ê±° 4ë…„ + íƒ€ê²Ÿ ë‚ ì§œ í¬í•¨)
    start_date = target_date - timedelta(days=365 * 4 + 30)
    start_date_str = start_date.strftime('%Y-%m-%d')
    
    logging.info(f"[PRICE-MODEL][{COMMODITY}] ðŸ” BQ ë°ì´í„° ì¡°íšŒ ë²”ìœ„: {start_date_str} ~ {target_date_str}")
    
    # Corn ë°ì´í„°
    q_corn = f"""
        SELECT time, close, volume as Volume, ema as EMA
        FROM `{PROJECT_ID}.{DATASET_ID}.{PRICE_TABLE}`
        WHERE DATE(time) >= '{start_date_str}' AND DATE(time) <= '{target_date_str}'
        ORDER BY time
    """
    df = client.query(q_corn).to_dataframe()
    
    if df.empty or df.iloc[-1]['time'].strftime('%Y-%m-%d') != target_date_str:
        logging.warning(f"[PRICE-MODEL][{COMMODITY}] âš ï¸ {target_date_str} ê°€ê²© ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ìž‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    logging.info(f"[PRICE-MODEL][{COMMODITY}] âœ… {PRICE_TABLE} ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(df)}ê±´")

    df['ds'] = pd.to_datetime(df['time'])
    df['y'] = pd.to_numeric(df['close'])
    
    # Soybean ë°ì´í„°
    q_soy = f"""
        SELECT time, close as soybean_close
        FROM `{PROJECT_ID}.{DATASET_ID}.soybean_price`
        WHERE DATE(time) >= '{start_date_str}' AND DATE(time) <= '{target_date_str}'
        ORDER BY time
    """
    try:
        df_soy = client.query(q_soy).to_dataframe()
        df_soy['time'] = pd.to_datetime(df_soy['time'])
        df = pd.merge(df, df_soy, on='time', how='left')
        logging.info(f"[PRICE-MODEL][{COMMODITY}] âœ… ë³´ì¡° ë°ì´í„°(Soybean) ë³‘í•© ì™„ë£Œ: {len(df_soy)}ê±´")
    except Exception as e:
        logging.warning(f"[PRICE-MODEL][{COMMODITY}] âš ï¸ Soybean ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        df['soybean_close'] = np.nan

    # 2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (Lag ìƒì„±)
    df['EMA_lag2'] = df['EMA'].shift(2)
    df['Volume_lag5'] = df['Volume'].shift(5)
    df['soybean_close_lag8'] = df['soybean_close'].shift(8)
    
    train_df = df[df['ds'] < target_date].dropna(subset=['EMA_lag2', 'Volume_lag5']).copy()
    
    use_soybean = False
    if 'soybean_close_lag8' in train_df.columns:
        if train_df['soybean_close_lag8'].notna().mean() > 0.5:
            use_soybean = True
            train_df = train_df.dropna(subset=['soybean_close_lag8'])
    
    regressors = ['EMA_lag2', 'Volume_lag5']
    if use_soybean: regressors.append('soybean_close_lag8')
        
    logging.info(f"[PRICE-MODEL][{COMMODITY}] ðŸ§  í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(train_df)}í–‰, Regressors: {regressors}")
    
    model = Prophet(seasonality_mode='multiplicative', yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)
    for reg in regressors: model.add_regressor(reg, mode='multiplicative')
    model.fit(train_df[['ds', 'y'] + regressors])
    
    # 4. ì˜ˆì¸¡
    future_row = df[df['ds'] == target_date].copy()
    if future_row[regressors].isna().any().any():
        future_row[regressors] = future_row[regressors].fillna(method='ffill')
        
    forecast = model.predict(future_row)
    
    # 5. ê²°ê³¼ ì •ë¦¬
    result = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper', 'trend']].iloc[0].to_dict()
    for col in ['weekly', 'yearly', 'extra_regressors_multiplicative']:
        if col in forecast.columns: result[col] = forecast[col].iloc[0]
    for reg in regressors:
        if reg in forecast.columns: result[f"{reg}_effect"] = forecast[reg].iloc[0]
            
    result['y'] = future_row['y'].iloc[0] 
    result['y_next'] = None
    result['Volume'] = future_row['Volume'].iloc[0]
    result['EMA'] = future_row['EMA'].iloc[0]
    result['corn_close'] = future_row['close'].iloc[0]
    result['soybean_close'] = future_row['soybean_close'].iloc[0]
    result['EMA_lag2'] = future_row['EMA_lag2'].iloc[0]
    result['Volume_lag5'] = future_row['Volume_lag5'].iloc[0]
    if use_soybean: result['soybean_close_lag8'] = future_row['soybean_close_lag8'].iloc[0]
    
    result['used_soybean'] = use_soybean
    result['used_corn'] = True
    result['volatility'] = result['yhat_upper'] - result['yhat_lower']
    result['ds'] = result['ds'].strftime('%Y-%m-%d')
    
    logging.info(f"[PRICE-MODEL][{COMMODITY}] âœ¨ í”¼ì²˜ ìƒì„± ì„±ê³µ: yhat={result['yhat']:.2f}, trend={result['trend']:.2f}")
    return result

def insert_to_bq(**context):
    """ìƒì„±ëœ í”¼ì²˜ë¥¼ BigQueryì— ì ìž¬"""
    feature_data = context['ti'].xcom_pull(task_ids='generate_features')
    
    if not feature_data:
        logging.info(f"[PRICE-MODEL][{COMMODITY}] â¹ï¸ ì ìž¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    client = get_bq_client()
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{TARGET_TABLE}"
    target_ds = feature_data['ds']
    
    # [ìƒì„¸ ë¡œê¹…] ì ìž¬ë  ë°ì´í„° ì „ì²´ ì¶œë ¥
    row = {k: (None if pd.isna(v) else v) for k, v in feature_data.items()}
    logging.info(f"[PRICE-MODEL][{COMMODITY}] ðŸ” ì ìž¬ ì˜ˆì • ì»¬ëŸ¼ ({len(row)}ê°œ): {list(row.keys())}")
    logging.info(f"[PRICE-MODEL][{COMMODITY}] ðŸ“„ ì ìž¬ ì˜ˆì • ë°ì´í„° ìƒì„¸:\n{json.dumps(row, indent=2, ensure_ascii=False)}")
    
    logging.info(f"[PRICE-MODEL][{COMMODITY}] ðŸ’¾ BQ ì ìž¬ ì‹œìž‘ (Target: {target_ds})")
    
    try:
        client.query(f"DELETE FROM `{table_id}` WHERE ds = '{target_ds}'").result()
        logging.info(f"[PRICE-MODEL][{COMMODITY}] ðŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ ({target_ds})")
    except Exception as e:
        logging.warning(f"[PRICE-MODEL][{COMMODITY}] âš ï¸ ì‚­ì œ ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")

    errors = client.insert_rows_json(table_id, [row])
    
    if errors:
        raise RuntimeError(f"BigQuery ì ìž¬ ì‹¤íŒ¨: {errors}")
        
    logging.info(f"[PRICE-MODEL][{COMMODITY}] âœ… ì ìž¬ ì™„ë£Œ ì„±ê³µ!")

with DAG(
    'generate_prophet_features_corn_v1',
    default_args=default_args,
    description='Corn Prophet í”¼ì²˜ ìƒì„± (Backfill ì§€ì›)',
    schedule_interval='30 16 * * *',
    catchup=True,
    max_active_runs=1,
    tags=['corn', 'prophet', 'feature_engineering']
) as dag:

    t1 = PythonOperator(task_id='generate_features', python_callable=generate_features, provide_context=True)
    t2 = PythonOperator(task_id='insert_to_bq', python_callable=insert_to_bq, provide_context=True)
    t1 >> t2
