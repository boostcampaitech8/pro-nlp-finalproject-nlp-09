from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import json
import logging
import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from google.cloud import bigquery

# [í™˜ê²½ ì„¤ì •]
PROJECT_ID = os.getenv("VERTEX_AI_PROJECT_ID", "project-5b75bb04-485d-454e-af7")
DATASET_ID = "tilda"
SOURCE_TABLE = "news_article"
TARGET_TABLE = "soybean_all_news_with_sentiment"
COMMODITY = "soybean"
MODEL_NAME = "ProsusAI/finbert"
START_DATE = datetime(2024, 1, 1)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': START_DATE,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def get_bq_client():
    return bigquery.Client(project=PROJECT_ID)

def run_sentiment_analysis(**context):
    """Airflow ì‹¤í–‰ ë‚ ì§œ(ds)ì˜ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    target_date_str = context['ds']
    logging.info(f"[NEWS-MODEL][{COMMODITY}] ðŸš€ Task ì‹œìž‘: run_sentiment_analysis (Target: {target_date_str})")
    client = get_bq_client()
    
    q_source = f"""
        SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}`
        WHERE filter_status = 'T'
          AND publish_date = '{target_date_str}'
          AND (LOWER(key_word) LIKE '%soybean%')
          AND (LOWER(key_word) LIKE '%price%' OR LOWER(key_word) LIKE '%demand%' OR LOWER(key_word) LIKE '%supply%' OR LOWER(key_word) LIKE '%inventory%')
    """
    df = client.query(q_source).to_dataframe()
    if df.empty:
        logging.info(f"[NEWS-MODEL][{COMMODITY}] â¹ï¸ {target_date_str}ì— ë¶„ì„í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìž‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return None

    logging.info(f"[NEWS-MODEL][{COMMODITY}] âœ… ë¶„ì„ ëŒ€ìƒ ë‰´ìŠ¤ ë¡œë“œ ì„±ê³µ: {len(df)}ê±´")
    logging.info(f"[NEWS-MODEL][{COMMODITY}] ðŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)
    model.eval()
    
    label_map = {0: 'positive', 1: 'negative', 2: 'neutral'}
    results = []

    logging.info(f"[NEWS-MODEL][{COMMODITY}] ðŸ§  ê°ì„± ë¶„ì„ ì‹œìž‘ (Device: {device})")
    for _, row in df.iterrows():
        text = f"{row['title']} {row.get('description', '')}".strip()
        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].cpu().numpy()
        
        pred_class = np.argmax(probs)
        res_row = row.to_dict()
        res_row['combined_text'] = text
        res_row['sentiment'] = label_map[pred_class]
        res_row['sentiment_confidence'] = float(probs[pred_class])
        res_row['positive_score'] = float(probs[0])
        res_row['negative_score'] = float(probs[1])
        res_row['neutral_score'] = float(probs[2])
        res_row['price_impact_score'] = float(probs[0] - probs[1])
        
        if isinstance(res_row['publish_date'], (datetime, pd.Timestamp)):
            res_row['publish_date'] = res_row['publish_date'].strftime('%Y-%m-%d')
        results.append(res_row)
    
    logging.info(f"[NEWS-MODEL][{COMMODITY}] âœ¨ ê°ì„± ë¶„ì„ ì™„ë£Œ (ê²°ê³¼: {len(results)}ê±´)")
    return results

def insert_to_bq(**context):
    """ê²°ê³¼ ì ìž¬ (ë™ì¼ ë‚ ì§œ ìž¬ì‹¤í–‰ ì‹œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)"""
    results = context['ti'].xcom_pull(task_ids='run_sentiment_analysis')
    if not results:
        logging.info(f"[NEWS-MODEL][{COMMODITY}] â¹ï¸ ì ìž¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    client = get_bq_client()
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{TARGET_TABLE}"
    target_date = context['ds']
    
    # [ìƒì„¸ ë¡œê¹…] ì ìž¬ ë°ì´í„° êµ¬ì¡° ë° ìƒ˜í”Œ ì¶œë ¥
    clean_results = [{k: (None if pd.isna(v) else v) for k, v in r.items()} for r in results]
    if clean_results:
        logging.info(f"[NEWS-MODEL][{COMMODITY}] ðŸ” ì ìž¬ ì˜ˆì • ì»¬ëŸ¼ ({len(clean_results[0])}ê°œ): {list(clean_results[0].keys())}")
        logging.info(f"[NEWS-MODEL][{COMMODITY}] ðŸ“„ ì²« ë²ˆì§¸ ê¸°ì‚¬ ì „ì²´ ë°ì´í„° ìƒ˜í”Œ:\n{json.dumps(clean_results[0], indent=2, ensure_ascii=False)}")
        logging.info(f"[NEWS-MODEL][{COMMODITY}] ðŸ“‹ ì „ì²´ ê¸°ì‚¬ ìš”ì•½ ({len(clean_results)}ê±´):")
        for i, r in enumerate(clean_results, 1):
            logging.info(f"  [{i}] ID: {r['id']} | Title: {r['title'][:50]}... | Sentiment: {r['sentiment']} | Impact: {r['price_impact_score']:.3f}")

    logging.info(f"[NEWS-MODEL][{COMMODITY}] ðŸ’¾ BQ ì ìž¬ ì‹œìž‘ (Target: {target_date})")
    try:
        client.query(f"DELETE FROM `{table_id}` WHERE publish_date = '{target_date}'").result()
        logging.info(f"[NEWS-MODEL][{COMMODITY}] ðŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ ({target_date})")
    except Exception: pass
    errors = client.insert_rows_json(table_id, clean_results)
    if errors: raise RuntimeError(f"BQ ì ìž¬ ì‹¤íŒ¨: {errors}")
    logging.info(f"[NEWS-MODEL][{COMMODITY}] âœ… ì ìž¬ ì™„ë£Œ ì„±ê³µ ({len(clean_results)}ê±´)")

with DAG(
    'process_news_sentiment_soybean_v1',
    default_args=default_args,
    description='Soybean ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (Backfill ì§€ì›)',
    schedule_interval='10 17 * * *', # ë§¤ì¼ 17:10
    catchup=True,
    max_active_runs=1,
    tags=['news', 'sentiment', 'soybean', 'finbert']
) as dag:
    t1 = PythonOperator(task_id='run_sentiment_analysis', python_callable=run_sentiment_analysis, provide_context=True)
    t2 = PythonOperator(task_id='insert_to_bq', python_callable=insert_to_bq, provide_context=True)
    t1 >> t2
