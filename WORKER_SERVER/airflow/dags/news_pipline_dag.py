from airflow import DAG
from airflow.models import Variable
from airflow.providers.standard.operators.python import PythonOperator
from datetime import datetime, timedelta
from hashlib import md5

import json
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ pathì— ì¶”ê°€ (crawler, processor ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•¨)
sys.path.append('/data/ephemeral/home/TeamProject-Repo/worker_server') 

from crawler.main_crawler import fetch_and_standardize
from processor.news_processor import NewsProcessor
from processor.embedder import TitanEmbedder
# í™˜ê²½ ì„¤ì • (Airflow Variables ìš°ì„ , ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜)
OPENAI_API_KEY = Variable.get("OPENAI_API_KEY", default_var=None) or os.getenv("OPENAI_API_KEY")
DATA_DIR = "/data/ephemeral/home/TeamProject-Repo/worker_server/data" # ë¡œì»¬ ë³¼ë¥¨ê³¼ ì—°ê²°ëœ ê²½ë¡œ

os.makedirs(os.path.join(DATA_DIR, 'raw'), exist_ok=True)
os.makedirs(os.path.join(DATA_DIR, 'processed'), exist_ok=True)

default_args = {
    'owner': 'sehun',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1), # ì‹œìž‘ì¼ ì„¤ì •
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'agri_news_pipeline_v1',
    default_args=default_args,
    description='ë†ì‚°ë¬¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° LLM ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸',
    schedule='@hourly', # ë§¤ì‹œê°„ ì‹¤í–‰
    catchup=False
) as dag:

    def crawl_task_func(**context):
        # execution_dateë¥¼ í™œìš©í•´ ì‹œê°„ë³„ íŒŒì¼ëª… ìƒì„±
        execution_date = context["ds_nodash"]
        hour = context["logical_date"].hour
        output_path = os.path.join(DATA_DIR, 'raw', f'news_{execution_date}_{hour}.json')
        
        # í¬ë¡¤ëŸ¬ ì‹¤í–‰
        results = fetch_and_standardize()
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        
        # ë‹¤ìŒ íƒœìŠ¤í¬ë¥¼ ìœ„í•´ ê²½ë¡œ ì „ë‹¬ (XCom)
        return output_path

    def process_task_func(**context):
        # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ìƒì„±ëœ íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜´
        input_path = context['ti'].xcom_pull(task_ids='crawl_news')
        output_path = os.path.join(DATA_DIR, 'processed', 'final_processed_news.json')
        
        # í”„ë¡œì„¸ì„œ ì‹¤í–‰
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY is not set")
        processor = NewsProcessor(api_key=OPENAI_API_KEY)
        processor.process_json_file(input_path=input_path, output_path=output_path)

    def embed_news_task_func(**context):
        """ìž‘ì„±í•˜ì‹  TitanEmbedderë¥¼ ì‚¬ìš©í•˜ì—¬ nullì¸ ìž„ë² ë”©ì„ ì±„ìš°ëŠ” í•¨ìˆ˜"""
        final_json_path = os.path.join(DATA_DIR, 'processed', 'final_processed_news.json')
        entity_json_path = os.path.join(DATA_DIR, 'processed', 'entity.json')
        triple_json_path = os.path.join(DATA_DIR, 'processed', 'triple.json')
        
        if not os.path.exists(final_json_path):
            print("âŒ ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # 1. íŒŒì¼ ì½ê¸°
        with open(final_json_path, 'r', encoding='utf-8') as f:
            all_articles = json.load(f)

        # 2. ìž„ë² ë” ì´ˆê¸°í™” (ìž‘ì„±í•˜ì‹  í´ëž˜ìŠ¤ ì‚¬ìš©)
        embedder = TitanEmbedder(region_name="us-east-1")
        newly_embedded_articles = []
        new_entities = []
        new_triples = []

        existing_entity_ids = set()
        existing_triple_ids = set()

        if os.path.exists(entity_json_path):
            with open(entity_json_path, 'r', encoding='utf-8') as f:
                try:
                    existing_entities = json.load(f)
                    existing_entity_ids = {e.get("hash_id") for e in existing_entities if e.get("hash_id")}
                except json.JSONDecodeError:
                    existing_entities = []
        else:
            existing_entities = []

        if os.path.exists(triple_json_path):
            with open(triple_json_path, 'r', encoding='utf-8') as f:
                try:
                    existing_triples = json.load(f)
                    existing_triple_ids = {t.get("hash_id") for t in existing_triples if t.get("hash_id")}
                except json.JSONDecodeError:
                    existing_triples = []
        else:
            existing_triples = []

        # 3. ìž„ë² ë”©ì´ ì—†ëŠ” ê¸°ì‚¬ë§Œ ê³¨ë¼ì„œ ìƒì„±
        for art in all_articles:
            # Tì¸ ê¸°ì‚¬ ì¤‘ ìž„ë² ë”©ì´ nullì¸ ê²½ìš°ë§Œ!
            if art.get('article_embedding') is None:
                text_to_embed = f"{art['title']}\n\n{art.get('description', '')}"
                
                print(f"ðŸ”„ ìž„ë² ë”© ìƒì„± ì¤‘: {art['title'][:20]}...")
                vector = embedder.generate_embedding(text_to_embed)
                
                if vector:
                    art['article_embedding'] = vector
                    newly_embedded_articles.append(art)

        # 4. ì—”í‹°í‹°/íŠ¸ë¦¬í”Œ ìž„ë² ë”© ìƒì„±
        def compute_mdhash_id(content: str, prefix: str) -> str:
            return prefix + md5(content.encode()).hexdigest()

        for art in newly_embedded_articles:
            for entity in art.get("named_entities", []) or []:
                entity_text = str(entity).strip()
                if not entity_text:
                    continue
                entity_id = compute_mdhash_id(entity_text, prefix="entity-")
                if entity_id in existing_entity_ids:
                    continue
                vector = embedder.generate_embedding(entity_text)
                if vector:
                    new_entities.append({
                        "hash_id": entity_id,
                        "entity_text": entity_text,
                        "embedding": vector,
                    })
                    existing_entity_ids.add(entity_id)

            for triple in art.get("triples", []) or []:
                triple_text = str(triple).strip()
                if not triple_text:
                    continue
                triple_id = compute_mdhash_id(triple_text, prefix="triple-")
                if triple_id in existing_triple_ids:
                    continue
                vector = embedder.generate_embedding(triple_text)
                if vector:
                    new_triples.append({
                        "hash_id": triple_id,
                        "triple_text": triple_text,
                        "embedding": vector,
                    })
                    existing_triple_ids.add(triple_id)

        # 5. íŒŒì¼ ì—…ë°ì´íŠ¸ ì €ìž¥
        if newly_embedded_articles:
            with open(final_json_path, 'w', encoding='utf-8') as f:
                json.dump(all_articles, f, indent=4, ensure_ascii=False)
            print(f"âœ… {len(newly_embedded_articles)}ê°œì˜ ë‰´ìŠ¤ ìž„ë² ë”© ì™„ë£Œ!")

        if new_entities:
            with open(entity_json_path, 'w', encoding='utf-8') as f:
                json.dump(existing_entities + new_entities, f, indent=4, ensure_ascii=False)
            print(f"âœ… {len(new_entities)}ê°œì˜ ì—”í‹°í‹° ìž„ë² ë”© ì™„ë£Œ!")

        if new_triples:
            with open(triple_json_path, 'w', encoding='utf-8') as f:
                json.dump(existing_triples + new_triples, f, indent=4, ensure_ascii=False)
            print(f"âœ… {len(new_triples)}ê°œì˜ íŠ¸ë¦¬í”Œ ìž„ë² ë”© ì™„ë£Œ!")


        return newly_embedded_articles

    # 1. í¬ë¡¤ë§ íƒœìŠ¤í¬
    crawl_news = PythonOperator(
        task_id='crawl_news',
        python_callable=crawl_task_func,
    )

    # 2. LLM ì²˜ë¦¬ íƒœìŠ¤í¬
    process_news = PythonOperator(
        task_id='process_news',
        python_callable=process_task_func,
    )
    # 3. ìž„ë² ë”© ìƒì„± íƒœìŠ¤í¬
    embed_news = PythonOperator(
        task_id='embed_news',
        python_callable=embed_news_task_func,
    )

    # íƒœìŠ¤í¬ ìˆœì„œ ì„¤ì •
    crawl_news >> process_news >> embed_news
