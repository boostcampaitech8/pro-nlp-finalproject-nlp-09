"""
ì˜¥ìˆ˜ìˆ˜ ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ì›ë³¸ ë°ì´í„°(news_articles_resources.csv)ë¥¼ í•„í„°ë§í•˜ê³  ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬
í•™ìŠµ ê°€ëŠ¥í•œ ë°ì´í„°(corn_all_news_with_sentiment.csv)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python preprocess_news_data.py \
        --input news_articles_resources.csv \
        --output corn_all_news_with_sentiment.csv
"""

import pandas as pd
import argparse
import re
from datetime import datetime
from finbert import (
    CommoditySentimentAnalyzer,
    prepare_text_for_analysis,
    get_sentiment_summary
)


def filter_corn_news(df, keyword_pattern=None):
    """
    ì˜¥ìˆ˜ìˆ˜ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§
    
    í•„í„°ë§ ì¡°ê±´:
    1. filter_status == 'T'
    2. key_wordì— corn AND (price OR demand OR supply OR inventory) í¬í•¨
    
    Args:
        df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        keyword_pattern: ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ íŒ¨í„´ (ì—†ìœ¼ë©´ ê¸°ë³¸ íŒ¨í„´ ì‚¬ìš©)
    
    Returns:
        DataFrame: í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
    """
    print("\n[1/5] ë‰´ìŠ¤ ë°ì´í„° í•„í„°ë§ ì¤‘...")
    print(f"ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ ê¸°ì‚¬")
    
    # 1. filter_status == 'T' í•„í„°ë§
    if 'filter_status' in df.columns:
        df_filtered = df[df['filter_status'] == 'T'].copy()
        print(f"  âœ“ filter_status='T' í•„í„°ë§: {len(df_filtered)}ê°œ ê¸°ì‚¬")
    else:
        print("  âš ï¸  'filter_status' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„° ì‚¬ìš©")
        df_filtered = df.copy()
    
    # 2. key_word í•„í„°ë§
    # corn AND (price OR demand OR supply OR inventory)
    if 'key_word' not in df_filtered.columns:
        print("  âš ï¸  'key_word' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ í•„í„°ë§ ìƒëµ")
        return df_filtered
    
    if keyword_pattern is None:
        # ê¸°ë³¸ íŒ¨í„´: cornì´ ìˆê³ , price/demand/supply/inventory ì¤‘ í•˜ë‚˜ ì´ìƒ í¬í•¨
        def matches_keyword(keyword_str):
            if pd.isna(keyword_str):
                return False
            
            keyword_lower = str(keyword_str).lower()
            
            # cornì´ ìˆëŠ”ì§€ í™•ì¸
            has_corn = 'corn' in keyword_lower
            
            # price, demand, supply, inventory ì¤‘ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸
            has_market_terms = any(term in keyword_lower for term in 
                                  ['price', 'demand', 'supply', 'inventory'])
            
            return has_corn and has_market_terms
    else:
        # ì»¤ìŠ¤í…€ íŒ¨í„´ ì‚¬ìš©
        def matches_keyword(keyword_str):
            if pd.isna(keyword_str):
                return False
            return bool(re.search(keyword_pattern, str(keyword_str), re.IGNORECASE))
    
    # í•„í„° ì ìš©
    mask = df_filtered['key_word'].apply(matches_keyword)
    df_filtered = df_filtered[mask].copy()
    
    print(f"  âœ“ í‚¤ì›Œë“œ í•„í„°ë§ ì™„ë£Œ: {len(df_filtered)}ê°œ ê¸°ì‚¬")
    print(f"    ì¡°ê±´: corn AND (price OR demand OR supply OR inventory)")
    
    return df_filtered


def validate_required_columns(df):
    """
    í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    
    Args:
        df: ê²€ì¦í•  ë°ì´í„°í”„ë ˆì„
    
    Returns:
        bool: ëª¨ë“  í•„ìˆ˜ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ True
    """
    required_cols = ['title', 'publish_date']
    optional_cols = ['description', 'all_text', 'article_embedding', 
                     'entity_embedding', 'triple_embedding', 'named_entities']
    
    missing_required = [col for col in required_cols if col not in df.columns]
    
    if missing_required:
        print(f"\nâŒ ì˜¤ë¥˜: í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_required}")
        return False
    
    missing_optional = [col for col in optional_cols if col not in df.columns]
    if missing_optional:
        print(f"\nâš ï¸  ê²½ê³ : ì¼ë¶€ ì„ íƒì  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_optional}")
        print("   í•™ìŠµ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    return True


def add_missing_columns(df):
    """
    í•™ìŠµì— í•„ìš”í•˜ì§€ë§Œ ì—†ëŠ” ì»¬ëŸ¼ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
    
    Returns:
        DataFrame: ì»¬ëŸ¼ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    df_result = df.copy()
    
    # ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
    embedding_cols = ['article_embedding', 'entity_embedding', 'triple_embedding']
    for col in embedding_cols:
        if col not in df_result.columns:
            print(f"  âš ï¸  '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            df_result[col] = None
    
    # named_entities ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
    if 'named_entities' not in df_result.columns:
        print(f"  âš ï¸  'named_entities' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
        df_result['named_entities'] = '{}'
    
    # descriptionì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
    if 'description' not in df_result.columns:
        print(f"  âš ï¸  'description' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
        df_result['description'] = ''
    
    return df_result


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ì˜¥ìˆ˜ìˆ˜ ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ (í•„í„°ë§ + ê°ì„± ë¶„ì„)'
    )
    
    # ì…ì¶œë ¥ íŒŒì¼
    parser.add_argument('--input', type=str, default='news_articles_resources.csv',
                       help='ì›ë³¸ ë‰´ìŠ¤ ë°ì´í„° CSV íŒŒì¼ (ê¸°ë³¸ê°’: news_articles_resources.csv)')
    parser.add_argument('--output', type=str, default='corn_all_news_with_sentiment.csv',
                       help='ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: corn_all_news_with_sentiment.csv)')
    
    # í•„í„°ë§ ì˜µì…˜
    parser.add_argument('--keyword_pattern', type=str, default=None,
                       help='ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ (ê¸°ë³¸ê°’: None, ìë™ íŒ¨í„´ ì‚¬ìš©)')
    parser.add_argument('--skip_filter', action='store_true',
                       help='í•„í„°ë§ì„ ê±´ë„ˆë›°ê³  ì „ì²´ ë°ì´í„° ì‚¬ìš©')
    
    # ê°ì„± ë¶„ì„ ì˜µì…˜
    parser.add_argument('--model_name', type=str, default='ProsusAI/finbert',
                       help='ê°ì„± ë¶„ì„ ëª¨ë¸ëª… (ê¸°ë³¸ê°’: ProsusAI/finbert)')
    parser.add_argument('--skip_sentiment', action='store_true',
                       help='ê°ì„± ë¶„ì„ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ë¶„ì„ëœ ë°ì´í„°ì¸ ê²½ìš°)')
    
    args = parser.parse_args()
    
    print("="*80)
    print("ì˜¥ìˆ˜ìˆ˜ ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
    print("="*80)
    print(f"\nì…ë ¥ íŒŒì¼: {args.input}")
    print(f"ì¶œë ¥ íŒŒì¼: {args.output}")
    print(f"ê°ì„± ë¶„ì„ ëª¨ë¸: {args.model_name}")
    
    # ========================================================================
    # STEP 1: ë°ì´í„° ë¡œë“œ
    # ========================================================================
    print(f"\n{'='*80}")
    print("STEP 1: ë°ì´í„° ë¡œë“œ")
    print(f"{'='*80}")
    
    try:
        df = pd.read_csv(args.input)
        print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬")
        print(f"  ì»¬ëŸ¼: {list(df.columns)}")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    if not validate_required_columns(df):
        return
    
    # ========================================================================
    # STEP 2: í•„í„°ë§
    # ========================================================================
    print(f"\n{'='*80}")
    print("STEP 2: ë‰´ìŠ¤ í•„í„°ë§")
    print(f"{'='*80}")
    
    if args.skip_filter:
        print("â­ï¸  í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤ (--skip_filter ì˜µì…˜)")
        df_filtered = df.copy()
    else:
        df_filtered = filter_corn_news(df, keyword_pattern=args.keyword_pattern)
        
        if len(df_filtered) == 0:
            print("\nâŒ ì˜¤ë¥˜: í•„í„°ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”.")
            return
    
    # ========================================================================
    # STEP 3: ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€
    # ========================================================================
    print(f"\n{'='*80}")
    print("STEP 3: ë°ì´í„° ê²€ì¦ ë° ë³´ì™„")
    print(f"{'='*80}")
    
    df_filtered = add_missing_columns(df_filtered)
    
    # ========================================================================
    # STEP 4: í…ìŠ¤íŠ¸ ì¤€ë¹„
    # ========================================================================
    print(f"\n{'='*80}")
    print("STEP 4: ê°ì„± ë¶„ì„ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„")
    print(f"{'='*80}")
    
    print("\n[2/5] í…ìŠ¤íŠ¸ ê²°í•© ì¤‘...")
    df_prepared = prepare_text_for_analysis(
        df_filtered,
        title_col='title',
        description_col='description',
        all_text_col='all_text' if 'all_text' in df_filtered.columns else None,
        output_col='combined_text'
    )
    print(f"  âœ“ combined_text ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ")
    
    # ========================================================================
    # STEP 5: ê°ì„± ë¶„ì„
    # ========================================================================
    print(f"\n{'='*80}")
    print("STEP 5: ê°ì„± ë¶„ì„")
    print(f"{'='*80}")
    
    if args.skip_sentiment:
        print("â­ï¸  ê°ì„± ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤ (--skip_sentiment ì˜µì…˜)")
        df_result = df_prepared.copy()
    else:
        print("\n[3/5] FinBERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
        analyzer = CommoditySentimentAnalyzer(model_name=args.model_name)
        
        print("\n[4/5] ê°ì„± ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        df_result = analyzer.analyze_dataframe(
            df_prepared,
            text_column='combined_text',
            show_progress=True
        )
        
        # ê°ì„± ë¶„ì„ ìš”ì•½
        print("\n[5/5] ê°ì„± ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
        summary = get_sentiment_summary(df_result)
        
        print(f"\nğŸ“Š ê°ì„± ë¶„í¬:")
        for sentiment, info in summary['sentiment_distribution'].items():
            print(f"  {sentiment:10s}: {info['count']:5d}ê°œ ({info['percentage']:5.1f}%)")
        
        print(f"\nğŸ“ˆ í†µê³„:")
        print(f"  í‰ê·  ì‹ ë¢°ë„:        {summary['avg_confidence']:.3f}")
        print(f"  í‰ê·  ê°€ê²© ì˜í–¥ë„:  {summary['avg_price_impact']:.3f}")
        print(f"  ì´ ê¸°ì‚¬ ìˆ˜:        {summary['total_count']:,}ê°œ")
    
    # ========================================================================
    # STEP 6: ì €ì¥
    # ========================================================================
    print(f"\n{'='*80}")
    print("STEP 6: ê²°ê³¼ ì €ì¥")
    print(f"{'='*80}")
    
    # ë‚ ì§œ í˜•ì‹ í™•ì¸ ë° ì •ë ¬
    if 'publish_date' in df_result.columns:
        df_result['publish_date'] = pd.to_datetime(df_result['publish_date'])
        df_result = df_result.sort_values('publish_date')
    
    # ì €ì¥
    df_result.to_csv(args.output, index=False, encoding='utf-8')
    print(f"\nâœ“ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.output}")
    print(f"  ìµœì¢… ê¸°ì‚¬ ìˆ˜: {len(df_result):,}ê°œ")
    print(f"  ì»¬ëŸ¼ ìˆ˜: {len(df_result.columns)}ê°œ")
    
    # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
    print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ìµœì‹  3ê°œ ê¸°ì‚¬):")
    sample_cols = ['publish_date', 'title', 'sentiment', 'price_impact_score']
    available_cols = [col for col in sample_cols if col in df_result.columns]
    print(df_result[available_cols].tail(3).to_string(index=False))
    
    # ========================================================================
    # ì™„ë£Œ
    # ========================================================================
    print(f"\n{'='*80}")
    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. ì „ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸: {args.output}")
    print(f"  2. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰:")
    print(f"     python train_models.py --news_path {args.output}")
    print()


if __name__ == "__main__":
    main()
