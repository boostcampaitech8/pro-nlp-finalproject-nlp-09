import pandas as pd
import numpy as np
from xgboost import XGBClassifier
import yaml
import os
import time
from typing import Dict, Any
import warnings
warnings.filterwarnings('ignore')


def load_config(config_path=None):
    """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    if config_path is None:
        # ê¸°ë³¸ ê²½ë¡œ: í˜„ì¬ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì˜ config.yaml
        base_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(base_dir, 'config.yaml')
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


class TimeSeriesXGBoostInference:
    """
    BigQueryì—ì„œ ê°€ì ¸ì˜¨ DataFrameì„ ì‚¬ìš©í•˜ì—¬ Walk-Forward ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” í´ë˜ìŠ¤
    inference.pyì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    def __init__(self, config_path=None):
        """
        ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            config_path (str, optional): config.yaml íŒŒì¼ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©.
        """
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.config = load_config(config_path)
        self.xgb_config = self.config['xgboost']
        self.validation_config = self.config['validation']
        
    def predict(self, history_df: pd.DataFrame, target_date: str) -> Dict[str, Any]:
        """
        ì œê³µëœ ê³¼ê±° ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ Walk-Forward ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì‹œì¥ ë°©í–¥ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        
        Args:
            history_df (pd.DataFrame): Prophet í”¼ì²˜ê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„.
                                    'ds' ì»¬ëŸ¼ê³¼ í•™ìŠµì— ì‚¬ìš©ëœ ëª¨ë“  í”¼ì²˜ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
                                    target_dateë¥¼ í¬í•¨í•œ ê³¼ê±° ë°ì´í„°ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
            target_date (str): ì˜ˆì¸¡í•  ë‚ ì§œ ë¬¸ìì—´ ('YYYY-MM-DD' í˜•ì‹).
            
        Returns:
            Dict: ì˜ˆì¸¡ ìƒì„¸ ê²°ê³¼ ì‚¬ì „ (inference.pyì™€ ë™ì¼í•œ í˜•ì‹).
        """
        print(f"\n{'='*70}")
        print(f"ğŸ¤– XGBoost ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œì‘")
        print(f"{'='*70}")
        print(f"ğŸ“… íƒ€ê²Ÿ ë‚ ì§œ: {target_date}")
        
        try:
            target_ts = pd.Timestamp(target_date)
        except ValueError:
            raise ValueError(f"ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ì…ë‹ˆë‹¤: {target_date}. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        # 'ds' ì»¬ëŸ¼ì´ datetime í˜•ì‹ì¸ì§€ í™•ì¸
        if not pd.api.types.is_datetime64_any_dtype(history_df['ds']):
            history_df['ds'] = pd.to_datetime(history_df['ds'])
        
        # íƒ€ê²Ÿ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” í–‰ ì°¾ê¸°
        target_idx = history_df[history_df['ds'] == target_ts].index
        
        if len(target_idx) == 0:
            raise ValueError(f"ì œê³µëœ ë°ì´í„°í”„ë ˆì„ì—ì„œ í•´ë‹¹ ë‚ ì§œ({target_date})ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        target_idx = target_idx[0]
        target_row_idx = history_df.index.get_loc(target_idx)
        
        # í”¼ì²˜ ì»¬ëŸ¼ ì •ì˜
        exclude_cols = ['ds', 'y', 'direction', 'y_change', 'yhat_lower', 'yhat_upper', 'EMA', 'Volume']
        feature_columns = [col for col in history_df.columns if col not in exclude_cols]
        
        print(f"\nğŸ“Š ë°ì´í„° ì¤€ë¹„")
        print(f"  - ì „ì²´ ë°ì´í„° í¬ê¸°: {len(history_df)} í–‰")
        print(f"  - ì‚¬ìš© í”¼ì²˜ ìˆ˜: {len(feature_columns)} ê°œ")
        
        # Walk-Forward í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        min_train_samples = self.validation_config['min_train_samples']
        window_size = self.validation_config.get('window_size', None)
        train_val_split = self.xgb_config['train_val_split']
        
        # íƒ€ê²Ÿ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš© (ë¯¸ë˜ ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        df_until_target = history_df.iloc[:target_row_idx + 1].copy()
        
        if len(df_until_target) < min_train_samples:
            raise ValueError(
                f"í•™ìŠµì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜({min_train_samples})ë³´ë‹¤ ì ìŠµë‹ˆë‹¤. "
                f"í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ ìˆ˜: {len(df_until_target)}"
            )
        
        # Sliding Window ì ìš©
        if window_size is None:
            train_val_start = 0
        else:
            train_val_start = max(0, target_row_idx - window_size)
        
        available_samples = target_row_idx - train_val_start
        train_size_relative = int(available_samples * train_val_split)
        train_end = train_val_start + train_size_relative
        
        # Train/Val/Test ë¶„ë¦¬
        X_train = df_until_target.iloc[train_val_start:train_end][feature_columns]
        y_train = df_until_target.iloc[train_val_start:train_end]['direction']
        
        X_val = df_until_target.iloc[train_end:target_row_idx][feature_columns]
        y_val = df_until_target.iloc[train_end:target_row_idx]['direction']
        
        X_test = df_until_target.iloc[target_row_idx:target_row_idx+1][feature_columns]
        row = df_until_target.iloc[target_row_idx:target_row_idx+1]
        
        print(f"\nğŸ“‚ ë°ì´í„° ë¶„í•  ì™„ë£Œ")
        print(f"  - Train ë°ì´í„°: {len(X_train)} í–‰")
        print(f"  - Val ë°ì´í„°: {len(X_val)} í–‰")
        print(f"  - Test ë°ì´í„°: {len(X_test)} í–‰")
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
        n_positive = (y_train == 1).sum()
        n_negative = (y_train == 0).sum()
        scale_pos_weight = n_negative / n_positive if n_positive > 0 else 1
        
        print(f"\nâš–ï¸  í´ë˜ìŠ¤ ë¶„í¬")
        print(f"  - ìƒìŠ¹(1): {n_positive} ê°œ ({n_positive/len(y_train)*100:.1f}%)")
        print(f"  - í•˜ë½(0): {n_negative} ê°œ ({n_negative/len(y_train)*100:.1f}%)")
        print(f"  - Scale Pos Weight: {scale_pos_weight:.3f}")
        
        # XGBoost íŒŒë¼ë¯¸í„° ì„¤ì •
        xgb_params = {
            'objective': self.xgb_config['objective'],
            'max_depth': self.xgb_config['max_depth'],
            'learning_rate': self.xgb_config['learning_rate'],
            'n_estimators': self.xgb_config['n_estimators'],
            'min_child_weight': self.xgb_config['min_child_weight'],
            'subsample': self.xgb_config['subsample'],
            'colsample_bytree': self.xgb_config['colsample_bytree'],
            'gamma': self.xgb_config['gamma'],
            'reg_alpha': self.xgb_config['reg_alpha'],
            'reg_lambda': self.xgb_config['reg_lambda'],
            'scale_pos_weight': scale_pos_weight,
            'random_state': self.xgb_config['random_state'],
            'verbosity': 0
        }
        
        # ëª¨ë¸ í•™ìŠµ
        early_stopping_rounds = self.xgb_config.get('early_stopping_rounds')
        
        print(f"\nğŸš€ XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"  - Max Depth: {xgb_params['max_depth']}")
        print(f"  - Learning Rate: {xgb_params['learning_rate']}")
        print(f"  - N Estimators: {xgb_params['n_estimators']}")
        
        train_start_time = time.time()
        
        if len(X_val) > 0 and early_stopping_rounds is not None:
            print(f"  - Early Stopping: {early_stopping_rounds} rounds")
            xgb_params['early_stopping_rounds'] = early_stopping_rounds
            xgb_model = XGBClassifier(**xgb_params)
            xgb_model.fit(
                X_train, y_train,
                eval_set=[(X_train, y_train), (X_val, y_val)],
                verbose=False
            )
        else:
            print(f"  - Early Stopping: ë¯¸ì‚¬ìš©")
            xgb_model = XGBClassifier(**xgb_params)
            xgb_model.fit(X_train, y_train)
        
        train_time = time.time() - train_start_time
        print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {train_time:.2f}ì´ˆ)")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        print(f"\nğŸ¯ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        prediction_prob = xgb_model.predict_proba(X_test)[0]  # [í•˜ë½í™•ë¥ , ìƒìŠ¹í™•ë¥ ]
        prediction = xgb_model.predict(X_test)[0]  # 0 ë˜ëŠ” 1
        
        # ì˜ˆì¸¡ ë°©í–¥ì— ë”°ë¥¸ ì‹ ë¢°ë„(í™•ë¥ ) ì¶”ì¶œ
        confidence = prediction_prob[1] if prediction == 1 else prediction_prob[0]
        
        # BigQueryì—ì„œ ê°€ì ¸ì˜¨ Prophet featuresë¥¼ ë°˜í™˜í•  ë”•ì…”ë„ˆë¦¬ ì¤€ë¹„
        result = {
            "target_date": target_date,
            "forecast_direction": "Up" if prediction == 1 else "Down",
            "confidence_score": float(confidence) * 100  # ì‹ ë¢°ë„ (%) ì¶”ê°€
        }
        
        # Prophet featureë“¤ì„ ê²°ê³¼ì— ì¶”ê°€
        # rowì˜ ëª¨ë“  ì»¬ëŸ¼ ê°’ì„ ê²°ê³¼ì— í¬í•¨
        for col in row.columns:
            if col not in ['ds']:  # dsëŠ” ì´ë¯¸ target_dateë¡œ í¬í•¨ë¨
                value = row[col].values[0]
                # NaN ì²´í¬ í›„ ë³€í™˜
                if pd.isna(value):
                    result[col] = None
                elif isinstance(value, (bool, np.bool_)):
                    # bool íƒ€ì…ì€ intë¡œ ë³€í™˜ (JSON ì§ë ¬í™” í˜¸í™˜)
                    result[col] = int(value)
                elif isinstance(value, (np.integer, np.floating, int, float)):
                    result[col] = float(value)
                else:
                    result[col] = value
        
        print(f"\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼")
        print(f"  - xgboost ì˜ˆì¸¡ ë°©í–¥: {result['forecast_direction']} ({'ìƒìŠ¹' if prediction == 1 else 'í•˜ë½'})")
        if 'yhat' in result and result['yhat'] is not None:
            print(f"  - Prophet ì˜ˆì¸¡ê°’: {result['yhat']:.2f}")
        if 'y' in result and result['y'] is not None:
            print(f"  - ì–´ì œ ì¢…ê°€: {result['y']:.2f}")
        print(f"{'='*70}\n")
        
        return result
