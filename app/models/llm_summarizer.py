from typing import Optional, List
from langchain_core.tools import tool
import subprocess
from google.auth import default
from google.auth.transport.requests import Request
from langchain_core.messages import HumanMessage, AIMessage

from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

from config.settings import (
    GENERATE_MODEL_NAME, GENERATE_MODEL_TEMPERATURE, GENERATE_MODEL_MAX_TOKENS,
    VERTEX_AI_PROJECT_ID, VERTEX_AI_LOCATION,
)
from models.timeseries_predictor import TimeSeriesPredictor
from models.sentiment_analyzer import SentimentAnalyzer


# ìƒìˆ˜ ì •ì˜
REPORT_FORMAT = """**ì¼ì¼ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ **
> **ğŸ“… ë¶„ì„ ì¼ì ** : (YYYY-MM-DD)
> **ğŸ’¬ ì¢…í•© ì˜ê²¬ ** : [ì¢…í•© ì˜ê²¬ í•œì¤„ ìš”ì•½]
---

### 1. ğŸ“Š ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ê°€ ì˜ê²¬
> [ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ í•œì¤„ í‰ê°€]

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì…ë ¥ ë°ì´í„° ê¸¸ì´** | Xì¼ (YYYY-MM-DD ë¶€í„° YYYY-MM-DDê¹Œì§€) |
| **ë§ˆì§€ë§‰ ê´€ì¸¡ê°’** | XXX.XX |
| **ì‹œê³„ì—´ ì˜ˆì¸¡ê°’** | XXX.XX |
| **ì‹ ë¢°ë„** | XX.XX % |

- **ì¶”ì„¸ ë¶„ì„**
  - [ìµœê·¼ ê¸°ê°„ í‰ê· ê³¼ ì „ ê¸°ê°„ í‰ê·  ë¹„êµ]
  - [ìµœê·¼ ë³€ë™ ì¶”ì´ ì„¤ëª…]
  - [ì‹œê³„ì—´ ì˜ˆì¸¡ê°’ í•´ì„ ë° ì‹ ë¢°ë„ í‰ê°€]

- **ì˜ˆì¸¡ê°’ í•´ì„**
  - [í˜„ì¬ ìˆ˜ì¤€ ëŒ€ë¹„ ì˜ˆì¸¡ê°’ ì˜ë¯¸]
  - [ë‹¨ê¸° ë³€ë™ì„± í‰ê°€]

---

### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„
> [ë‰´ìŠ¤ ê¸°ì‚¬ ê°ì„±ë¶„ì„ í•œì¤„ í‰ê°€]

| ê¸°ì‚¬ ë²ˆí˜¸ | ë‚´ìš© ìš”ì•½ | ê°ì„± |
|-----------|-----------|------|
| 1 | [ê¸°ì‚¬ ìš”ì•½] | ê¸ì •/ë¶€ì •/ì¤‘ë¦½ |
| 2 | [ê¸°ì‚¬ ìš”ì•½] | ê¸ì •/ë¶€ì •/ì¤‘ë¦½ |
| ... | ... | ... |

- **ê°ì„± ë¹„ìœ¨**
  - ê¸ì •: Xê°œ (XX %)
  - ë¶€ì •: Xê°œ (XX %)
  - ì¤‘ë¦½: Xê°œ (XX %)
  - **ì¢…í•© ê°ì„±**: ê¸ì •/ë¶€ì •/ì¤‘ë¦½

- **í…ìŠ¤íŠ¸ì  ê·¼ê±°**
  - [ê° ê¸°ì‚¬ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„]

---

### 3. ë¯¸ë˜ ì‹œì¥ ì „ë§

| êµ¬ë¶„ | ê·¼ê±° | ì „ë§ |
|------|------|------|
| **ë‹¨ê¸°(1â€“3ì¼)** | [ê·¼ê±° ìš”ì•½] | **[ì „ë§]** [ìƒì„¸ ì„¤ëª…] |
| **ì¤‘ê¸°(1ì£¼)** | [ê·¼ê±° ìš”ì•½] | **[ì „ë§]** [ìƒì„¸ ì„¤ëª…] |
| **ì¥ê¸°(1ê°œì›”)** | [ê·¼ê±° ìš”ì•½] | **[ì „ë§]** [ìƒì„¸ ì„¤ëª…] |

- **ìœ„í—˜ ìš”ì¸**
  - [ì£¼ìš” ìœ„í—˜ ìš”ì¸ ë‚˜ì—´]

- **ê¸°íšŒ ìš”ì¸**
  - [ì£¼ìš” ê¸°íšŒ ìš”ì¸ ë‚˜ì—´]

---

### 4. ì¢…í•© ì˜ê²¬

- **[í˜„ì¬ ì‹œì¥ ìƒí™© ìš”ì•½]**
- **[ì£¼ìš” ì§€í‘œ ë° ë‰´ìŠ¤ ìš”ì•½]**
- **[ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° ì „ë§ ìš”ì•½]**
- **[íˆ¬ìì ì…ì¥ì—ì„œì˜ ì¡°ì–¸]**

**ê²°ë¡ **: [ë‚ ì§œ] ê¸°ì¤€, ì‹œì¥ì€ **[ì „ë§]**ì„ ìœ ì§€í•  ê²ƒìœ¼ë¡œ ì „ë§ë˜ë©°, **[ì£¼ìš” ì„±ì¥ ë™ë ¥]**ì´ ì£¼ìš” ì„±ì¥ ë™ë ¥ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ **[ì£¼ìš” ë¦¬ìŠ¤í¬]**ì— ë”°ë¥¸ ë¦¬ìŠ¤í¬ë¥¼ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¤‘ìš”**: 
- ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
- í‘œ í˜•ì‹ì€ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ì„¹ì…˜ ë²ˆí˜¸ì™€ ì œëª©ì€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
- ê° ì„¹ì…˜ì€ "---"ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
- ì–¸ì–´ëŠ” ë°˜ë“œì‹œ ìˆœìˆ˜ í•œêµ­ì–´(í•œê¸€)ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."""

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

**ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬**:
1. timeseries_predictor: ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡
   - data: ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìˆ«ì ë¬¸ìì—´ (ì˜ˆ: "100.5,101.2,102.3")

2. news_sentiment_analyzer: ë‰´ìŠ¤ ê¸°ì‚¬ ê°ì„±ë¶„ì„
   - texts: "[ê¸°ì‚¬ 1]\\në‚´ìš©\\n\\n[ê¸°ì‚¬ 2]\\në‚´ìš©" í˜•ì‹ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë¬¸ìì—´

**ë„êµ¬ ì‚¬ìš© ê·œì¹™**:
- ì‚¬ìš©ì ì…ë ¥ì— ì œê³µëœ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ timeseries_predictor ë„êµ¬ì— ì „ë‹¬í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
- ì‚¬ìš©ì ì…ë ¥ì— ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ news_sentiment_analyzer ë„êµ¬ì— ì „ë‹¬í•˜ì—¬ ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
- ê° ë„êµ¬ëŠ” í•„ìš”í•œ ë§Œí¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°›ì€ í›„ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

**ë³´ê³ ì„œ ì‘ì„± í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤)**:

""" + REPORT_FORMAT


# LangChain Tools ì •ì˜
@tool
def timeseries_predictor(data: str) -> str:
    """
    ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
    Args:
        data: ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìˆ«ì ë¬¸ìì—´ (ì˜ˆ: "100.5,101.2,102.3")
    
    Returns:
        ì˜ˆì¸¡ê°’ê³¼ ì‹ ë¢°ë„ë¥¼ í¬í•¨í•œ ë¶„ì„ ê²°ê³¼
    """
    data_list = [float(x.strip()) for x in data.split(",") if x.strip()]
    predictor = TimeSeriesPredictor()
    prediction, confidence = predictor.predict(data_list)
    
    result = f"""
ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼:
- ì˜ˆì¸¡ê°’: {prediction:.2f}
- ì‹ ë¢°ë„: {confidence:.2%}
- ì…ë ¥ ë°ì´í„° ê¸¸ì´: {len(data_list)}
"""
    return result.strip()


def _format_sentiment_results(text_list: List[str], results: List[dict]) -> str:
    """ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ë°˜í™˜"""
    sentiment_map = {"positive": "ê¸ì •", "negative": "ë¶€ì •", "neutral": "ì¤‘ë¦½"}
    
    # ê°ì„±ë³„ ê°œìˆ˜ ê³„ì‚°
    counts = {
        "positive": sum(1 for r in results if r.get("sentiment") == "positive"),
        "negative": sum(1 for r in results if r.get("sentiment") == "negative"),
        "neutral": sum(1 for r in results if r.get("sentiment") == "neutral"),
    }
    total = len(results)
    
    # ê¸°ì‚¬ë³„ ìƒì„¸ ê²°ê³¼ ìƒì„±
    detailed_results = []
    for i, (text, result) in enumerate(zip(text_list, results), 1):
        sentiment_en = result.get("sentiment", "neutral")
        sentiment_ko = sentiment_map.get(sentiment_en, "ì¤‘ë¦½")
        detailed_results.append(f"ê¸°ì‚¬ {i}: [{sentiment_ko}] {text}")
    
    # ì¢…í•© ê°ì„± ê²°ì •
    if counts["positive"] > counts["negative"]:
        overall = "ê¸ì •"
    elif counts["negative"] > counts["positive"]:
        overall = "ë¶€ì •"
    else:
        overall = "ì¤‘ë¦½"
    
    return f"""ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼:
- ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {total}ê°œ
- ê¸ì •: {counts['positive']}ê°œ ({counts['positive']/total*100:.1f}%)
- ë¶€ì •: {counts['negative']}ê°œ ({counts['negative']/total*100:.1f}%)
- ì¤‘ë¦½: {counts['neutral']}ê°œ ({counts['neutral']/total*100:.1f}%)
- ì¢…í•© ê°ì„±: {overall}

ê¸°ì‚¬ë³„ ê°ì„± ë¶„ì„:
{chr(10).join(detailed_results)}
""".strip()


@tool
def news_sentiment_analyzer(texts: str) -> str:
    """
    FinBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        texts: "[ê¸°ì‚¬ 1]\\në‚´ìš©\\n\\n[ê¸°ì‚¬ 2]\\në‚´ìš©" í˜•ì‹ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë¬¸ìì—´
    
    Returns:
        ê° ê¸°ì‚¬ì˜ ê°ì„± ë¶„ì„ ê²°ê³¼ì™€ ì¢…í•© ê°ì„±
    """
    import re
    
    # [ê¸°ì‚¬ N] íŒ¨í„´ìœ¼ë¡œ ê¸°ì‚¬ ì¶”ì¶œ (ë¼ë²¨ê³¼ ë‚´ìš©ì„ í•¨ê»˜ í•˜ë‚˜ì˜ ê¸°ì‚¬ë¡œ ì¸ì‹)
    article_pattern = r'\[ê¸°ì‚¬\s*\d+\]\s*\n(.+?)(?=\n\n\[ê¸°ì‚¬\s*\d+\]|$)'
    matches = re.finditer(article_pattern, texts, re.DOTALL)
    
    text_list = []
    for match in matches:
        article_text = match.group(1).strip()
        if article_text:
            text_list.append(article_text)
    
    if not text_list:
        return "ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    analyzer = SentimentAnalyzer()
    results = analyzer.analyze_batch(text_list)
    return _format_sentiment_results(text_list, results)


class LLMSummarizer:
    """Vertex AIë¥¼ ì‚¬ìš©í•˜ëŠ” LangChain Agentë¥¼ ì´ìš©í•œ í†µí•© ë¶„ì„"""
    
    def __init__(
        self, 
        model_name: str = None,
        project_id: str = None,
        location: str = None
    ):
        """
        Args:
            model_name: ìƒì„± ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì˜ GENERATE_MODEL_NAME)
            project_id: Google Cloud í”„ë¡œì íŠ¸ ID (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì„¤ì • íŒŒì¼ ë˜ëŠ” gcloud configì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜´)
            location: Vertex AI ë¦¬ì „ (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì˜ VERTEX_AI_LOCATION)
        """
        self.model_name = model_name or GENERATE_MODEL_NAME
        self.project_id = project_id or VERTEX_AI_PROJECT_ID or self._get_project_id()
        self.location = location or VERTEX_AI_LOCATION
        self.llm = None
        self.agent = None
        self._initialize()
    
    def _get_project_id(self) -> str:
        """gcloud configì—ì„œ í”„ë¡œì íŠ¸ IDë¥¼ ê°€ì ¸ì˜´"""
        try:
            result = subprocess.run(
                ["gcloud", "config", "get-value", "project"],
                capture_output=True,
                text=True,
                timeout=2
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip()
            else:
                raise ValueError("gcloud configì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            raise ValueError(
                f"project_idê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
                f"í•´ê²° ë°©ë²•: gcloud config set project YOUR_PROJECT_ID\n"
                f"ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ GOOGLE_CLOUD_PROJECTë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n"
                f"ì˜¤ë¥˜: {e}"
            )
    
    def _get_access_token(self) -> str:
        """Google Cloud ì¸ì¦ í† í° ê°€ì ¸ì˜¤ê¸°"""
        credentials, _ = default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
        if not credentials.valid:
            credentials.refresh(Request())
        return credentials.token
    
    def _build_base_url(self) -> str:
        """Vertex AI OpenAI í˜¸í™˜ API base URL ìƒì„±"""
        return (
            f"https://{self.location}-aiplatform.googleapis.com/v1/"
            f"projects/{self.project_id}/locations/{self.location}/endpoints/openapi"
        )
    
    def _create_llm(self, access_token: str) -> ChatOpenAI:
        """ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return ChatOpenAI(
            model=self.model_name,
            base_url=self._build_base_url(),
            api_key=access_token,
            temperature=GENERATE_MODEL_TEMPERATURE,
            max_tokens=GENERATE_MODEL_MAX_TOKENS,
            model_kwargs={
                "parallel_tool_calls": False,
            },
        )
    
    
    def _initialize(self):
        """LLM ë° Agent ì´ˆê¸°í™”"""
        access_token = self._get_access_token()
        self.llm = self._create_llm(access_token)
        print(f"âœ… ChatOpenAI (Vertex AI OpenAI í˜¸í™˜ API) ì‚¬ìš©: {self.model_name}")
        
        tools = [
            timeseries_predictor,
            news_sentiment_analyzer
        ]
        llm_with_tools = self.llm.bind_tools(tools)
        
        self.agent = create_agent(
            model=llm_with_tools,
            tools=tools,
            system_prompt=SYSTEM_PROMPT,
        )
    
    def _build_user_input(
        self,
        context: str,
        timeseries_table_id: Optional[str] = None,
        timeseries_value_column: Optional[str] = None,
        timeseries_days: Optional[int] = None,
        news_table_id: Optional[str] = None,
        news_value_column: Optional[str] = None,
        news_days: Optional[int] = None,
        timeseries_data: Optional[List[float]] = None,
        news_texts: Optional[List[str]] = None
    ) -> str:
        """Agentì—ê²Œ ì „ë‹¬í•  ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ ìƒì„±
        
        Args:
            context: ë¶„ì„ ë§¥ë½
            timeseries_data: ì§ì ‘ ì „ë‹¬í•  ì‹œê³„ì—´ ë°ì´í„°
            news_texts: ì§ì ‘ ì „ë‹¬í•  ë‰´ìŠ¤ í…ìŠ¤íŠ¸
            ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        """
        user_input = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ë¶„ì„ ë§¥ë½**: {context or "ìµœê·¼ ì‹œì¥ ìƒí™© ë¶„ì„"}

"""
        
        # ì‹œê³„ì—´ ë°ì´í„° ì§ì ‘ í¬í•¨
        if timeseries_data:
            data_str = ", ".join(map(str, timeseries_data))
            user_input += f"**ì‹œê³„ì—´ ë°ì´í„°**: {data_str}\n\n"
            user_input += "- ì´ ë°ì´í„°ë¥¼ timeseries_predictor ë„êµ¬ì— ì „ë‹¬í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n\n"
        
        # ë‰´ìŠ¤ ê¸°ì‚¬ ì§ì ‘ í¬í•¨
        if news_texts:
            texts_str = "\n\n".join([f"[ê¸°ì‚¬ {i+1}]\n{text}" for i, text in enumerate(news_texts)])
            user_input += f"**ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬**:\n{texts_str}\n\n"
            user_input += "- ì´ ë°ì´í„°ë¥¼ news_sentiment_analyzer ë„êµ¬ì— ì „ë‹¬í•˜ì—¬ ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n"
        
        return user_input
    
    def _validate_output_format(self, summary: str) -> bool:
        """ì¶œë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦ (ìµœì†Œ ê²€ì¦)
        
        Returns:
            bool: í˜•ì‹ì´ ì˜¬ë°”ë¥´ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False
        """
        # ìµœì†Œí•œì˜ ê¸¸ì´ í™•ì¸
        if not summary or len(summary.strip()) < 100:
            return False
        
        return True
    
    def _extract_summary_from_result(self, result: dict) -> str:
        """Agent ì‹¤í–‰ ê²°ê³¼ì—ì„œ ìš”ì•½ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        import json
        messages = result.get("messages", [])
        
        # messagesì—ì„œ ë§ˆì§€ë§‰ AIMessageì˜ content ì¶”ì¶œ
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                content = str(msg.content) if msg.content else ""
                content = content.strip().rstrip('\\')
                
                # JSON í˜•ì‹ì˜ tool call argumentsëŠ” ê±´ë„ˆë›°ê¸°
                if content.startswith("{") and content.strip().endswith("}"):
                    try:
                        # JSON íŒŒì‹± ì‹œë„
                        parsed = json.loads(content)
                        # tool call arguments í˜•ì‹ì¸ì§€ í™•ì¸ (texts, data ë“±ì˜ í‚¤ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°)
                        if isinstance(parsed, dict) and any(key in parsed for key in ["texts", "data"]):
                            continue
                    except (json.JSONDecodeError, ValueError):
                        # JSONì´ ì•„ë‹ˆë©´ ê³„ì† ì§„í–‰
                        pass
                
                # GPT-OSS-20B íŠ¹ìˆ˜ í˜•ì‹ ì œê±° (<|channel|> ë“±)
                # tool calling í˜•ì‹ì¸ ê²½ìš° ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if content.startswith("<|channel|>") and "<|call|>" in content:
                    # tool calling í˜•ì‹ì´ê³  ì‹¤ì œ ë³´ê³ ì„œ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    if not any(keyword in content for keyword in ["ë³´ê³ ì„œ", "ë¶„ì„", "ì˜ê²¬", "ì „ë§", "ì‹œì¥"]):
                        continue
                
                if content and len(content) > 50:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ
                    return content
        
        # messagesì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° output í•„ë“œ í™•ì¸
        output = result.get("output") or result.get("final_output")
        if output:
            return str(output).strip().rstrip('\\')
        
        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ì „ì²´ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        return str(result).strip().rstrip('\\')
    
    def summarize(
        self,
        context: str = "",
        timeseries_table_id: Optional[str] = None,
        timeseries_value_column: Optional[str] = None,
        timeseries_days: Optional[int] = None,
        news_table_id: Optional[str] = None,
        news_value_column: Optional[str] = None,
        news_days: Optional[int] = None,
        timeseries_data: Optional[List[float]] = None,
        news_texts: Optional[List[str]] = None,
        max_retries: int = 2,
    ) -> dict:
        """LangChain Agentë¥¼ ì´ìš©í•œ LLM ìš”ì•½ ìƒì„±
        
        Args:
            context: ë¶„ì„ ë§¥ë½ (ì‹œê°„ ë²”ìœ„, ì‹œì¥ ìƒí™© ë“±)
            timeseries_table_id: ì‹œê³„ì—´ ë°ì´í„° í…Œì´ë¸”ëª… (ê¸°ë³¸ê°’: "corn_price")
            timeseries_value_column: ì‹œê³„ì—´ ê°’ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: "close")
            timeseries_days: ì‹œê³„ì—´ ë°ì´í„° ê°€ì ¸ì˜¬ ì¼ìˆ˜ (ê¸°ë³¸ê°’: 30)
            news_table_id: ë‰´ìŠ¤ í…Œì´ë¸”ëª… (ê¸°ë³¸ê°’: "news_article")
            news_value_column: ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: "description")
            news_days: ë‰´ìŠ¤ ê°€ì ¸ì˜¬ ì¼ìˆ˜ (ê¸°ë³¸ê°’: 3)
            timeseries_data: ì‹œê³„ì—´ ì˜ˆì¸¡ì— ì‚¬ìš©í•  ë°ì´í„° (í•˜ìœ„ í˜¸í™˜ì„±, ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
            news_texts: ê°ì„±ë¶„ì„ì— ì‚¬ìš©í•  ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±, ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
            max_retries: ì¶œë ¥ í˜•ì‹ì´ ë§ì§€ ì•Šì„ ë•Œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 2)
        
        Returns:
            dict: {
                'summary': str,  # LLM ìš”ì•½
                'agent_result': dict,  # Agent ì‹¤í–‰ ê²°ê³¼ ì „ì²´ (Tool ë©”ì‹œì§€ í¬í•¨)
            }
        """
        user_input = self._build_user_input(
            context=context,
            timeseries_table_id=timeseries_table_id,
            timeseries_value_column=timeseries_value_column,
            timeseries_days=timeseries_days,
            news_table_id=news_table_id,
            news_value_column=news_value_column,
            news_days=news_days,
            timeseries_data=timeseries_data,
            news_texts=news_texts
        )
        
        for attempt in range(max_retries + 1):
            # Agent ì‹¤í–‰ (LangChainì´ ìë™ìœ¼ë¡œ tool callì„ ì²˜ë¦¬í•¨)
            if attempt == 0:
                result = self.agent.invoke({
                    "messages": [HumanMessage(content=user_input)]
                })
            else:
                # ì¬ì‹œë„ ì‹œ ê¸°ì¡´ ë©”ì‹œì§€ ì‚¬ìš©
                result = self.agent.invoke({
                    "messages": result.get('messages', [])
                })
            
            # ê²°ê³¼ ì¶”ì¶œ
            if isinstance(result, dict):
                messages = result.get('messages', [])
                
                summary = self._extract_summary_from_result(result)
                agent_result = result
                
                # ë””ë²„ê¹…: ë©”ì‹œì§€ ìƒíƒœ í™•ì¸
                print(f"\n[ë””ë²„ê¹…] ì´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
                tool_call_count = sum(1 for msg in messages if isinstance(msg, AIMessage) and msg.tool_calls)
                tool_result_count = sum(1 for msg in messages if hasattr(msg, 'name') and msg.name)
                print(f"  Tool í˜¸ì¶œ: {tool_call_count}íšŒ, Tool ê²°ê³¼: {tool_result_count}ê°œ")
            else:
                summary = str(result).strip().rstrip('\\')
                agent_result = {'messages': []}
            
            # ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° í™•ì¸
            if not summary or len(summary.strip()) < 50:
                print(f"\nâš ï¸ ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ê¸¸ì´: {len(summary)}ì)")
                # ë§ˆì§€ë§‰ AIMessageì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì°¾ê¸°
                if isinstance(result, dict):
                    messages = result.get('messages', [])
                    for msg in reversed(messages):
                        if isinstance(msg, AIMessage) and msg.content:
                            content = str(msg.content)
                            # GPT-OSS-20B íŠ¹ìˆ˜ í˜•ì‹ì´ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì°¾ê¸°
                            if "<|channel|>" not in content and len(content.strip()) > 50:
                                summary = content.strip()
                                print(f"  â†’ ëŒ€ì²´ í…ìŠ¤íŠ¸ ë°œê²¬ (ê¸¸ì´: {len(summary)}ì)")
                                break
            
            # ì¶œë ¥ í˜•ì‹ ê²€ì¦
            if summary and len(summary.strip()) > 50 and self._validate_output_format(summary):
                return {
                    'summary': summary or '',
                    'agent_result': agent_result,
                }
            
            # í˜•ì‹ì´ ë§ì§€ ì•Šìœ¼ë©´ ì¬ì‹œë„
            if attempt < max_retries:
                print(f"\nâš ï¸ ì¶œë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                print(f"í˜„ì¬ ìš”ì•½ ê¸¸ì´: {len(summary)}ì")
                if summary:
                    print(f"ìš”ì•½ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):\n{summary[:500]}...\n")
                
                user_input = f"""{user_input}

**ì¤‘ìš”**: ì´ì „ ì‘ë‹µì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”:

{REPORT_FORMAT}

**íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”**:
1. ì„¹ì…˜ ì œëª©ì´ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤: "### 1. ğŸ“Š ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ê°€ ì˜ê²¬", "### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„", "### 3. ë¯¸ë˜ ì‹œì¥ ì „ë§", "### 4. ì¢…í•© ì˜ê²¬"
2. ê° ì„¹ì…˜ì€ "---"ë¡œ êµ¬ë¶„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤ (ìµœì†Œ 3ê°œ)
3. ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹(|)ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
4. í—¤ë”ì— "ğŸ“… ë¶„ì„ ì¼ì"ì™€ "ğŸ’¬ ì¢…í•© ì˜ê²¬"ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
5. Tool í˜¸ì¶œ í›„ ë°˜ë“œì‹œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤"""
            else:
                print("\nâš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜•ì‹ì´ ì™„ë²½í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print(f"ìµœì¢… ìš”ì•½ ê¸¸ì´: {len(summary)}ì")
                if summary:
                    print(f"ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°: {summary[:200]}...")
                print("ê²€ì¦ì„ í†µê³¼í•˜ì§€ ëª»í–ˆì§€ë§Œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        
        return {
            'summary': summary or '',
            'agent_result': agent_result,
        }