from typing import Optional
from langchain_core.tools import tool
import subprocess
import json
from langchain_core.messages import HumanMessage, AIMessage

from langchain.agents import create_agent
from langchain_google_vertexai import ChatVertexAI

from config.settings import (
    GENERATE_MODEL_NAME,
    GENERATE_MODEL_TEMPERATURE,
    GENERATE_MODEL_MAX_TOKENS,
    VERTEX_AI_PROJECT_ID,
    VERTEX_AI_LOCATION,
)
from models.timeseries_predictor import predict_market_trend
from models.sentiment_analyzer import SentimentAnalyzer
from models.keyword_analyzer import analyze_keywords as _analyze_keywords
from models.pastnews_rag_runner import run_pastnews_rag as _run_pastnews_rag


# ìƒìˆ˜ ì •ì˜
REPORT_FORMAT = """**ì¼ì¼ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ **
> **ğŸ“… ë¶„ì„ ì¼ì ** : (YYYY-MM-DD)
> **ğŸ’¬ ì¢…í•© ì˜ê²¬ ** : [ì¢…í•© ì˜ê²¬ í•œì¤„ ìš”ì•½]
---

### 1. ğŸ“Š ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ê°€ ì˜ê²¬
> [ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ í•œì¤„ í‰ê°€]

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì…ë ¥ ë°ì´í„° ê¸¸ì´** | 5ë…„ (Prophet Features) |
| **ë§ˆì§€ë§‰ ê´€ì¸¡ê°’** | [Last Observed Value] |
| **ì‹œê³„ì—´ ì˜ˆì¸¡ê°’** | [Forecast Value] |
| **ì‹ ë¢°ë„** | [Confidence Score] % |

- **ì¶”ì„¸ ë¶„ì„**
  - **ìµœê·¼ ê¸°ê°„ í‰ê· ** (7ì¼) : [Recent Mean]
  - **ì „ ê¸°ê°„ í‰ê· ** : [All-time Mean]
  - **ìµœê·¼ ë³€ë™ ì¶”ì´** : [Trend Analysis: Rising/Falling ë“± ì„¤ëª…]
  - **ì‹œê³„ì—´ ì˜ˆì¸¡ê°’ í•´ì„** : [Forecast Direction] ë°©í–¥ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë©°, ì‹ ë¢°ë„ëŠ” [Confidence Score]% ì…ë‹ˆë‹¤.

- **ì˜ˆì¸¡ê°’ í•´ì„**
  - **í˜„ì¬ ìˆ˜ì¤€ ëŒ€ë¹„** : [Last Value] ëŒ€ë¹„ [Forecast Value] ë¡œ ë³€ë™ ì˜ˆìƒ.
  - **ë‹¨ê¸° ë³€ë™ì„± í‰ê°€** : ë³€ë™ì„± ì§€í‘œ [Volatility Index] ìˆ˜ì¤€.

---

### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„
> [ë‰´ìŠ¤ ê¸°ì‚¬ ê°ì„±ë¶„ì„ í•œì¤„ í‰ê°€]

- **ì£¼ìš” ë‰´ìŠ¤ (evidence_news)**
  - news_sentiment_analyzer ë„êµ¬ ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ ì•„ë˜ í‘œ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.
  - **ì¤‘ìš”**: titleê³¼ all_textê°€ ì˜ì–´ë¡œ ë˜ì–´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”.
  
  | ë²ˆí˜¸ | ë‰´ìŠ¤ ì œëª© | ì˜í–¥ë ¥ ì ìˆ˜ | ìš”ì•½ |
  |-----------|-----------|-------------|------|
  | 1 | [ë‰´ìŠ¤ ì œëª©(í•œêµ­ì–´ ë²ˆì—­)] | [ì ìˆ˜] | [ë‚´ìš© ìš”ì•½] |
  | 2 | [ë‰´ìŠ¤ ì œëª©(í•œêµ­ì–´ ë²ˆì—­)] | [ì ìˆ˜] | [ë‚´ìš© ìš”ì•½] |
  | ... | ... | ... | ... |

- **ì‹œì¥ ì˜í–¥ë ¥ ë¶„ì„**
  - **ìƒìŠ¹ í™•ë¥ **: [Probability] %
  - **ì¢…í•© ì˜ê²¬**: [ë‰´ìŠ¤ ê¸°ë°˜ ìƒìŠ¹/í•˜ë½ ì˜ˆì¸¡ ì˜ê²¬]

- **í…ìŠ¤íŠ¸ì  ê·¼ê±°**
  - [ê° ë‰´ìŠ¤ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„]
  - **ì£¼ìš” í‚¤ì›Œë“œ**: [keyword_analyzer ê²°ê³¼ì˜ top_entities ìƒìœ„ 10ê°œ entity]

- **ê³¼ê±° ê´€ë ¨ ë‰´ìŠ¤**
  - pastnews_rag ë„êµ¬ ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ ì•„ë˜ í‘œ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.
  - **ì¤‘ìš”**: descriptionì´ ì˜ì–´ë¡œ ë˜ì–´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ "ë‰´ìŠ¤ ì œëª©" ì»¬ëŸ¼ì— í‘œì‹œí•˜ì„¸ìš”.
  
  | ë‰´ìŠ¤ ë‚ ì§œ | ë‰´ìŠ¤ ë‚´ìš© | ë‹¹ì¼ | 1ì¼í›„ | 3ì¼í›„ |
  |-----------|-----------|------|------|------|
  | [ë‰´ìŠ¤ ë‚ ì§œ] | [ë‰´ìŠ¤ ë‚´ìš©(í•œêµ­ì–´ ë²ˆì—­)] | [price_0ì¼í›„] | [price_1ì¼í›„] | [price_3ì¼í›„] |
  | [ë‰´ìŠ¤ ë‚ ì§œ] | [ë‰´ìŠ¤ ë‚´ìš©(í•œêµ­ì–´ ë²ˆì—­)] | [price_0ì¼í›„] | [price_1ì¼í›„] | [price_3ì¼í›„] |
  | ... | ... | ... | ... | ... |

---

### 3. ì¢…í•© ì˜ê²¬

- **[í˜„ì¬ ì‹œì¥ ìƒí™© ìš”ì•½]**
- **[ì£¼ìš” ì§€í‘œ ë° ë‰´ìŠ¤ ìš”ì•½]**
- **[ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° ì „ë§ ìš”ì•½]**
- **[íˆ¬ìì ì…ì¥ì—ì„œì˜ ì¡°ì–¸]**

**ê²°ë¡ **: [ë‚ ì§œ] ê¸°ì¤€, ì‹œì¥ì€ **[ì „ë§]**ì„ ìœ ì§€í•  ê²ƒìœ¼ë¡œ ì „ë§ë˜ë©°, **[ì£¼ìš” ì„±ì¥ ë™ë ¥]**ì´ ì£¼ìš” ì„±ì¥ ë™ë ¥ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ **[ì£¼ìš” ë¦¬ìŠ¤í¬]**ì— ë”°ë¥¸ ë¦¬ìŠ¤í¬ë¥¼ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¤‘ìš”**: 
- ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
- í‘œ í˜•ì‹ì€ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ì„¹ì…˜ ë²ˆí˜¸ì™€ ì œëª©ì€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
- ê° ì„¹ì…˜ì€ "---"ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
- ì£¼ìš” í‚¤ì›Œë“œëŠ” #í‚¤ì›Œë“œ1 #í‚¤ì›Œë“œ2 í˜•ì‹ìœ¼ë¡œ í‘œê¸°
- ë‰´ìŠ¤ ê´€ë ¨ ë‚´ìš©ì´ ì˜ì–´ë¡œ ë˜ì–´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”. ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”.
- ì–¸ì–´ëŠ” ë°˜ë“œì‹œ ìˆœìˆ˜ í•œêµ­ì–´(í•œê¸€)ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."""

SYSTEM_PROMPT = (
    """ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

**ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬**:
1. timeseries_predictor: ì‹œê³„ì—´ ë°ì´í„° ê¸°ë°˜ ì‹œì¥ ì˜ˆì¸¡
   - target_date: ë¶„ì„í•  ëŒ€ìƒ ë‚ ì§œ (í˜•ì‹: "YYYY-MM-DD")
   - ì„¤ëª…: ì§€ì •ëœ ë‚ ì§œì˜ ê°€ê²© ì¶”ì„¸, ì˜ˆì¸¡ê°’, ì‹ ë¢°ë„ ë“±ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

2. news_sentiment_analyzer: ë‰´ìŠ¤ ê¸°ë°˜ ì‹œì¥ ì˜í–¥ë ¥ ë¶„ì„ ë° ê·¼ê±° ì¶”ì¶œ
   - target_date: ë¶„ì„í•  ëŒ€ìƒ ë‚ ì§œ (í˜•ì‹: "YYYY-MM-DD")
   - ì„¤ëª…: í•´ë‹¹ ë‚ ì§œ ì „í›„ì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œì¥ ìƒìŠ¹/í•˜ë½ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ê³ , ì˜ˆì¸¡ì˜ í•µì‹¬ ê·¼ê±°ê°€ ëœ ì£¼ìš” ë‰´ìŠ¤ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

3. keyword_analyzer: ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (Entity Confidence / PageRank ê¸°ë°˜)
   - target_date: ë¶„ì„í•  ëŒ€ìƒ ë‚ ì§œ (í˜•ì‹: "YYYY-MM-DD")
   - days: ë¶„ì„í•  ì¼ìˆ˜ (ê¸°ë³¸ 3ì¼)
   - ì„¤ëª…: PageRank ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ ë‰´ìŠ¤ì˜ Entity Confidence(ì¤‘ìš”ë„) ìƒìœ„ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
   - ë°˜í™˜ ê°’: top_entities (ìƒìœ„ 10ê°œ, ê° í•­ëª©: {"entity": "...", "score": ...})

4. pastnews_rag: ì „ë‹¬ë°›ì€ triplesë¡œ ìœ ì‚¬ ë‰´ìŠ¤ descriptionê³¼ publish_date ì¡°íšŒ
   - triples_json: keyword_analyzer ê²°ê³¼ì˜ top_triplesì—ì„œ ê° í•­ëª©ì˜ "triple" ë°°ì—´ë§Œ ëª¨ì€ JSON ë¬¸ìì—´. ì˜ˆ: [["United States","experiencing","government shutdown"], ...]
   - top_k: ìœ ì‚¬ hash_id ê°œìˆ˜ (ê¸°ë³¸ 5)
   - ì„¤ëª…: keyword_analyzer í˜¸ì¶œ í›„, ê·¸ ê²°ê³¼ì˜ top_triplesë¥¼ triples_json ì¸ìë¡œ ë„˜ê²¨ì„œ í˜¸ì¶œí•˜ì„¸ìš”. ìœ ì‚¬í•œ tripleì„ ê°€ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ descriptionê³¼ publish_dateë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

**ë„êµ¬ ì‚¬ìš© ê·œì¹™**:
- ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ(target_date)ê°€ ì£¼ì–´ì§€ë©´ ë°˜ë“œì‹œ `timeseries_predictor`, `news_sentiment_analyzer`, `keyword_analyzer`ë¥¼ ëª¨ë‘ í˜¸ì¶œí•œ ë’¤, keyword_analyzer ê²°ê³¼ì˜ top_triplesë¥¼ triples_json ì¸ìë¡œ ë„˜ê²¨ `pastnews_rag(triples_json=..., top_k=5)`ë¥¼ í•œ ë²ˆ í˜¸ì¶œí•˜ì„¸ìš”.
- ì´ì „ ë„êµ¬ê°€ ì˜¤ë¥˜ë¥¼ ë°˜í™˜í•˜ë”ë¼ë„, ì„¸ ë„êµ¬ë¥¼ ë°˜ë“œì‹œ ëª¨ë‘ í˜¸ì¶œí•œ ë’¤ì—ë§Œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
- `news_sentiment_analyzer` ê²°ê³¼ì— í¬í•¨ëœ 'evidence_news'ëŠ” ë³´ê³ ì„œì˜ '### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„' ì„¹ì…˜ì˜ 'ì£¼ìš” ë‰´ìŠ¤ (evidence_news)' í•­ëª©ì— ì•„ë˜ í‘œ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”. **titleê³¼ all_textê°€ ì˜ì–´ë¡œ ë˜ì–´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”.**
  | ë²ˆí˜¸ | ë‰´ìŠ¤ ì œëª© | ì˜í–¥ë ¥ ì ìˆ˜ | ìš”ì•½ |
  |-----------|-----------|-------------|------|
  | [ë²ˆí˜¸] | [ë‰´ìŠ¤ ì œëª©(í•œêµ­ì–´ ë²ˆì—­)] | [ì ìˆ˜] | [ë‚´ìš© ìš”ì•½(í•œêµ­ì–´ ë²ˆì—­)] |
- `pastnews_rag` ë„êµ¬ ê²°ê³¼(article_info)ëŠ” ë°˜ë“œì‹œ '### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„' ì„¹ì…˜ ë‚´ 'ê³¼ê±° ê´€ë ¨ ë‰´ìŠ¤ (pastnews_rag)' í•­ëª©ì— ì•„ë˜ í‘œ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”. **descriptionì´ ì˜ì–´ë¡œ ë˜ì–´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ "ë‰´ìŠ¤ ì œëª©" ì»¬ëŸ¼ì— í‘œì‹œí•˜ì„¸ìš”.**
  | ë‰´ìŠ¤ ë‚ ì§œ | ë‰´ìŠ¤ ì œëª© | ë‹¹ì¼ | 1ì¼í›„ | 3ì¼í›„ |
  |-----------|-----------|------|------|------|
  | [ë‰´ìŠ¤ ë‚ ì§œ] | [ë‰´ìŠ¤ ì œëª©(í•œêµ­ì–´ ë²ˆì—­)] | [price_0ì¼í›„] | [price_1ì¼í›„] | [price_3ì¼í›„] |
- `keyword_analyzer` ê²°ê³¼ì˜ top_entitiesë¥¼ í™œìš©í•  ë•Œ: (1) scoreëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. (2) entity ì´ë¦„ë§Œ ì‚¬ìš©í•˜ì—¬ ìµœëŒ€í•œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. (3) #í‚¤ì›Œë“œ1 #í‚¤ì›Œë“œ2 í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”. ì˜ˆ: #ì˜¥ìˆ˜ìˆ˜ #ê°€ê²© #ìˆ˜ì¶œ #ë¯¸êµ­ë†ë¬´ë¶€ #ì‹œì¥
- ì„¸ ë„êµ¬ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë…¼ë¦¬ì ì¸ ê¸ˆìœµ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ì‹œê³„ì—´ ì§€í‘œ, ë‰´ìŠ¤ ê°ì„± ë¶„ì„, í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ê°€ ì„œë¡œ ë³´ì™„ë˜ë„ë¡ ì„œìˆ í•˜ì„¸ìš”.
- target_dateëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ë¬¸ìì—´ ë¦¬í„°ëŸ´ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”. (YYYY-MM-DD)
**ë³´ê³ ì„œ ì‘ì„± í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤)**:

"""
    + REPORT_FORMAT
)


# LangChain Tools ì •ì˜
@tool
def timeseries_predictor(target_date: str) -> str:
    """
    íŠ¹ì • ë‚ ì§œì˜ ê¸ˆìœµ ì‹œì¥ ì¶”ì„¸(ìƒìŠ¹/í•˜ë½)ì™€ ê°€ê²©ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

    Args:
        target_date: ë¶„ì„í•  ë‚ ì§œ ë¬¸ìì—´ (í˜•ì‹: "YYYY-MM-DD")

    Returns:
        JSON í˜•ì‹ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë¬¸ìì—´ (ì˜ˆì¸¡ê°’, ë°©í–¥, ì‹ ë¢°ë„, ì¶”ì„¸ ë¶„ì„ ë“± í¬í•¨)
    """
    return predict_market_trend(target_date)


@tool
def news_sentiment_analyzer(target_date: str) -> str:
    """
    íŠ¹ì • ë‚ ì§œì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œì¥ ì˜í–¥ë ¥ì„ ì˜ˆì¸¡í•˜ê³  ì£¼ìš” ê·¼ê±° ë‰´ìŠ¤(ì œëª©, ì˜í–¥ë ¥ ì ìˆ˜, ê´€ê³„ ì •ë³´ ë“±)ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    Args:
        target_date: ë¶„ì„í•  ë‚ ì§œ ë¬¸ìì—´ (í˜•ì‹: "YYYY-MM-DD")

    Returns:
        JSON í˜•ì‹ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë¬¸ìì—´ (ìƒìŠ¹ í™•ë¥ , ê·¼ê±° ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸, í”¼ì²˜ ìš”ì•½ í¬í•¨)
    """
    analyzer = SentimentAnalyzer()
    result = analyzer.predict_market_impact(target_date)
    return json.dumps(result, ensure_ascii=False)


@tool
def keyword_analyzer(target_date: str, days: int = 3) -> str:
    """
    íŠ¹ì • ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    PageRank ì•Œê³ ë¦¬ì¦˜(Entity Confidence)ê³¼ ì„ë² ë”© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì„ í™œìš©í•˜ì—¬ í•µì‹¬ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        target_date: ë¶„ì„í•  ë‚ ì§œ ë¬¸ìì—´ (í˜•ì‹: "YYYY-MM-DD")
        days: ë¶„ì„í•  ì¼ìˆ˜ (ê¸°ë³¸ 3ì¼, ìµœëŒ€ 7ì¼ ê¶Œì¥)

    Returns:
        JSON: top_entities (ìƒìœ„ 10ê°œ), top_triples (í•µì‹¬ ì—”í‹°í‹°ê°€ í¬í•¨ëœ triple ì¤‘ ì—£ì§€ ì‹¤ì œ weightÃ—entity PageRank ì¤‘ìš”ë„ ìƒìœ„ 10ê°œ, ê° í•­ëª©: {"triple": [s,v,o], "importance": ì ìˆ˜})
    """
    result = json.loads(_analyze_keywords(target_date=target_date, days=days, top_k=10))
    top_entities = result.get("top_entities", [])[:10]
    top_triples = result.get("top_triples", [])
    return json.dumps({"top_entities": top_entities, "top_triples": top_triples}, ensure_ascii=False, indent=2)


@tool
def pastnews_rag(triples_json: str, top_k: int = 5) -> str:
    """
    ì „ë‹¬ë°›ì€ triples(triples_json)ë¡œ ìœ ì‚¬ ë‰´ìŠ¤ hash_id ê²€ìƒ‰ ë° í•´ë‹¹ ë‰´ìŠ¤ì˜ descriptionê³¼ publish_dateë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    keyword_analyzer í˜¸ì¶œ í›„, ê·¸ ê²°ê³¼ì˜ top_triplesì—ì„œ ê° í•­ëª©ì˜ "triple" ë°°ì—´ë§Œ ëª¨ì•„ JSON ë¬¸ìì—´ë¡œ ë„˜ê¸°ì„¸ìš”.

    Args:
        triples_json: triples ë°°ì—´ì˜ JSON ë¬¸ìì—´. ê° tripleì€ [ì£¼ì–´, ë™ì‚¬, ëª©ì ì–´]. ì˜ˆ: [["United States","experiencing","government shutdown"], ...]
        top_k: ìœ ì‚¬ hash_id ê°œìˆ˜ (ê¸°ë³¸ 5)

    Returns:
        JSON: article_info (ê° í•­ëª©: {"description": str, "publish_date": str, "price_0ì¼í›„": float, "price_1ì¼í›„": float, "price_3ì¼í›„": float}), error(ìˆì„ ê²½ìš°)
    """
    triples = []
    try:
        parsed = json.loads(triples_json)
        if isinstance(parsed, list):
            for item in parsed:
                if isinstance(item, (list, tuple)) and len(item) >= 3:
                    triples.append(list(item[:3]))
                elif isinstance(item, dict) and "triple" in item and isinstance(item["triple"], (list, tuple)) and len(item["triple"]) >= 3:
                    triples.append(list(item["triple"][:3]))
    except (json.JSONDecodeError, TypeError):
        pass
    result = _run_pastnews_rag(triples=triples if triples else None, top_k=top_k)
    return json.dumps(result, ensure_ascii=False, indent=2)


class LLMSummarizer:
    """Vertex AIë¥¼ ì‚¬ìš©í•˜ëŠ” LangChain Agentë¥¼ ì´ìš©í•œ í†µí•© ë¶„ì„"""

    def __init__(self, model_name: str = None, project_id: str = None, location: str = None):
        """
        Args:
            model_name: ìƒì„± ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì˜ GENERATE_MODEL_NAME)
            project_id: Google Cloud í”„ë¡œì íŠ¸ ID (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì„¤ì • íŒŒì¼ ë˜ëŠ” gcloud configì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜´)
            location: Vertex AI ë¦¬ì „ (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì˜ VERTEX_AI_LOCATION)
        """
        self.model_name = model_name or GENERATE_MODEL_NAME
        self.project_id = project_id or VERTEX_AI_PROJECT_ID or self._get_project_id()
        self.location = location or VERTEX_AI_LOCATION
        self.llm = None
        self.agent = None
        self._initialize()

    # TODO project id .envë¡œ ê´€ë¦¬
    def _get_project_id(self) -> str:
        """gcloud configì—ì„œ í”„ë¡œì íŠ¸ IDë¥¼ ê°€ì ¸ì˜´"""
        try:
            result = subprocess.run(
                ["gcloud", "config", "get-value", "project"], capture_output=True, text=True, timeout=2
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip()
            else:
                raise ValueError("gcloud configì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            raise ValueError(
                f"project_idê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
                f"í•´ê²° ë°©ë²•: gcloud config set project YOUR_PROJECT_ID\n"
                f"ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ GOOGLE_CLOUD_PROJECTë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n"
                f"ì˜¤ë¥˜: {e}"
            )

    def _create_llm(self) -> ChatVertexAI:
        """ChatVertexAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ëª¨ë¸ëª…Â·í”„ë¡œì íŠ¸Â·ë¦¬ì „ì€ envì—ì„œ ë¡œë“œ)"""
        return ChatVertexAI(
            model=self.model_name,
            project=self.project_id,
            location=self.location,
            temperature=GENERATE_MODEL_TEMPERATURE,
            max_output_tokens=GENERATE_MODEL_MAX_TOKENS,
        )

    def _initialize(self):
        """LLM ë° Agent ì´ˆê¸°í™”"""
        self.llm = self._create_llm()
        print(f"âœ… ChatVertexAI ì‚¬ìš© (ëª¨ë¸: {self.model_name}, env ê¸°ë°˜)")

        tools = [timeseries_predictor, news_sentiment_analyzer, keyword_analyzer, pastnews_rag]
        llm_with_tools = self.llm.bind_tools(tools)

        self.agent = create_agent(
            model=llm_with_tools,
            tools=tools,
            system_prompt=SYSTEM_PROMPT,
        )

    def _build_user_input(
        self,
        context: str,
        target_date: str,
    ) -> str:
        """Agentì—ê²Œ ì „ë‹¬í•  ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ ìƒì„±"""

        user_input = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ë¶„ì„ ë§¥ë½**: {context or "ìµœê·¼ ì‹œì¥ ìƒí™© ë¶„ì„"}
**ë¶„ì„ ê¸°ì¤€ ì¼ì**: {target_date}

ìœ„ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œëœ ê·œì¹™ê³¼ í˜•ì‹ì„ ë”°ë¼ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        return user_input

    def _validate_output_format(self, summary: str) -> bool:
        """ì¶œë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦ (ìµœì†Œ ê²€ì¦)

        Returns:
            bool: í˜•ì‹ì´ ì˜¬ë°”ë¥´ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False
        """
        # ìµœì†Œí•œì˜ ê¸¸ì´ í™•ì¸
        if not summary or len(summary.strip()) < 100:
            return False

        return True

    def _normalize_ai_content(self, content) -> str:
        """Vertex AI ë“±ì—ì„œ contentê°€ [{'type': 'text', 'text': '...'}, ...] í˜•íƒœì¼ ë•Œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
        if content is None:
            return ""
        # ë¦¬ìŠ¤íŠ¸(part í˜•ì‹)ì¸ ê²½ìš°
        if isinstance(content, list):
            parts = []
            for part in content:
                if isinstance(part, dict) and part.get("type") == "text" and "text" in part:
                    parts.append(str(part["text"]))
            if parts:
                return "\n".join(parts)
        # ë¬¸ìì—´ë¡œ ì§ë ¬í™”ëœ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì˜ˆ: "[{'type': 'text', 'text': '...'}]")
        if isinstance(content, str) and content.strip().startswith("[") and "'text'" in content:
            try:
                import ast
                parsed = ast.literal_eval(content)
                if isinstance(parsed, list):
                    parts = []
                    for part in parsed:
                        if isinstance(part, dict) and part.get("type") == "text" and "text" in part:
                            parts.append(str(part["text"]))
                    if parts:
                        return "\n".join(parts)
            except (ValueError, SyntaxError):
                pass
        return str(content)

    def _extract_summary_from_result(self, result: dict) -> str:
        """Agent ì‹¤í–‰ ê²°ê³¼ì—ì„œ ìš”ì•½ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        import json

        messages = result.get("messages", [])

        # messagesì—ì„œ ë§ˆì§€ë§‰ AIMessageì˜ content ì¶”ì¶œ
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                raw = msg.content
                content = self._normalize_ai_content(raw)
                content = content.strip().rstrip("\\")

                # JSON í˜•ì‹ì˜ tool call argumentsëŠ” ê±´ë„ˆë›°ê¸°
                if content.startswith("{{") and content.strip().endswith("}}"):
                    try:
                        # JSON íŒŒì‹± ì‹œë„
                        parsed = json.loads(content)
                        # tool call arguments í˜•ì‹ì¸ì§€ í™•ì¸ (texts, data ë“±ì˜ í‚¤ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°)
                        if isinstance(parsed, dict) and any(key in parsed for key in ["texts", "data", "target_date"]):
                            continue
                    except (json.JSONDecodeError, ValueError):
                        # JSONì´ ì•„ë‹ˆë©´ ê³„ì† ì§„í–‰
                        pass

                # GPT-OSS-20B íŠ¹ìˆ˜ í˜•ì‹ ì œê±° (<|channel|> ë“±)
                # tool calling í˜•ì‹ì¸ ê²½ìš° ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if content.startswith("<|channel|>") and "<|call|>" in content:
                    # tool calling í˜•ì‹ì´ê³  ì‹¤ì œ ë³´ê³ ì„œ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    if not any(keyword in content for keyword in ["ë³´ê³ ì„œ", "ë¶„ì„", "ì˜ê²¬", "ì „ë§", "ì‹œì¥"]):
                        continue

                if content and len(content) > 50:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ
                    return content

        # messagesì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° output í•„ë“œ í™•ì¸
        output = result.get("output") or result.get("final_output")
        if output:
            return str(output).strip().rstrip("\\")

        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ì „ì²´ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        return str(result).strip().rstrip("\\")

    # TODO ì¬ì‹œë„ ë¡œì§ ì ê²€
    def summarize(
        self,
        context: str = "",
        target_date: Optional[str] = None,
        max_retries: int = 2,
    ) -> dict:
        """LangChain Agentë¥¼ ì´ìš©í•œ LLM ìš”ì•½ ìƒì„±

        Args:
            context: ë¶„ì„ ë§¥ë½
            target_date: ë¶„ì„ ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD)
            max_retries: ì¬ì‹œë„ íšŸìˆ˜
        """
        # ë‚ ì§œ ê¸°ë³¸ê°’ (ì˜¤ëŠ˜)
        if not target_date:
            from datetime import datetime

            target_date = datetime.now().strftime("%Y-%m-%d")

        user_input = self._build_user_input(context=context, target_date=target_date)

        for attempt in range(max_retries + 1):
            # Agent ì‹¤í–‰ (LangChainì´ ìë™ìœ¼ë¡œ tool callì„ ì²˜ë¦¬í•¨)
            if attempt == 0:
                result = self.agent.invoke({"messages": [HumanMessage(content=user_input)]})
            else:
                # ì¬ì‹œë„ ì‹œ ê¸°ì¡´ ë©”ì‹œì§€ ì‚¬ìš©
                result = self.agent.invoke({"messages": result.get("messages", [])})

            # ê²°ê³¼ ì¶”ì¶œ
            if isinstance(result, dict):
                messages = result.get("messages", [])

                summary = self._extract_summary_from_result(result)
                agent_result = result

                # ë””ë²„ê¹…: ë©”ì‹œì§€ ìƒíƒœ í™•ì¸
                print(f"\n[ë””ë²„ê¹…] ì´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
                tool_call_count = sum(1 for msg in messages if isinstance(msg, AIMessage) and msg.tool_calls)
                tool_result_count = sum(1 for msg in messages if hasattr(msg, "name") and msg.name)
                print(f"  Tool í˜¸ì¶œ: {tool_call_count}íšŒ, Tool ê²°ê³¼: {tool_result_count}ê°œ")
            else:
                summary = str(result).strip().rstrip("\\")
                agent_result = {"messages": []}

            # ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° í™•ì¸
            if not summary or len(summary.strip()) < 50:
                print(f"\nâš ï¸ ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ê¸¸ì´: {len(summary)}ì)")
                # ë§ˆì§€ë§‰ AIMessageì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì°¾ê¸° (Vertex AI part í˜•ì‹ í¬í•¨)
                if isinstance(result, dict):
                    messages = result.get("messages", [])
                    for msg in reversed(messages):
                        if isinstance(msg, AIMessage) and msg.content:
                            content = self._normalize_ai_content(msg.content)
                            if "<|channel|>" not in content and len(content.strip()) > 50:
                                summary = content.strip()
                                print(f"  â†’ ëŒ€ì²´ í…ìŠ¤íŠ¸ ë°œê²¬ (ê¸¸ì´: {len(summary)}ì)")
                                break

            # ì¶œë ¥ í˜•ì‹ ê²€ì¦
            if summary and len(summary.strip()) > 50 and self._validate_output_format(summary):
                return {
                    "summary": summary or "",
                    "agent_result": agent_result,
                }

            # í˜•ì‹ì´ ë§ì§€ ì•Šìœ¼ë©´ ì¬ì‹œë„
            if attempt < max_retries:
                print(f"\nâš ï¸ ì¶œë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                print(f"í˜„ì¬ ìš”ì•½ ê¸¸ì´: {len(summary)}ì")
                if summary:
                    print(f"ìš”ì•½ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):\n{summary[:500]}...\n")

                user_input = f"""{user_input}

**ì¤‘ìš”**: ì´ì „ ì‘ë‹µì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”:

{REPORT_FORMAT}

**íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”**:
1. ì„¹ì…˜ ì œëª©ì´ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤: "### 1. ğŸ“Š ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ê°€ ì˜ê²¬", "### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„", "### 3. ì¢…í•© ì˜ê²¬"
2. ê° ì„¹ì…˜ì€ "---"ë¡œ êµ¬ë¶„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤ (ìµœì†Œ 3ê°œ)
3. ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹(|)ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
4. í—¤ë”ì— "ğŸ“… ë¶„ì„ ì¼ì"ì™€ "ğŸ’¬ ì¢…í•© ì˜ê²¬"ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
5. Tool í˜¸ì¶œ í›„ ë°˜ë“œì‹œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤"""
            else:
                print("\nâš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜•ì‹ì´ ì™„ë²½í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print(f"ìµœì¢… ìš”ì•½ ê¸¸ì´: {len(summary)}ì")
                if summary:
                    print(f"ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°: {summary[:200]}...")
                print("ê²€ì¦ì„ í†µê³¼í•˜ì§€ ëª»í–ˆì§€ë§Œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")

        return {
            "summary": summary or "",
            "agent_result": agent_result,
        }
