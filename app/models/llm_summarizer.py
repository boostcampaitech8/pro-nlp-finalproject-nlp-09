"""
LLM ê¸°ë°˜ ê¸ˆìœµ ë³´ê³ ì„œ ìƒì„± ëª¨ë“ˆ

Vertex AIì™€ LangChainì„ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ ì˜ˆì¸¡ ë° ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼
ì¢…í•©í•œ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import json
import logging
from typing import Optional

from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

from libs.gcp import GCPServiceFactory
from libs.utils.config import get_config

from models.timeseries_predictor import predict_market_trend
from models.sentiment_analyzer import SentimentAnalyzer

logger = logging.getLogger(__name__)

# ì„¤ì • ë¡œë“œ
_config = get_config()


# ìƒìˆ˜ ì •ì˜
REPORT_FORMAT = """**ì¼ì¼ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ **
> **ğŸ“… ë¶„ì„ ì¼ì ** : (YYYY-MM-DD)
> **ğŸ’¬ ì¢…í•© ì˜ê²¬ ** : [ì¢…í•© ì˜ê²¬ í•œì¤„ ìš”ì•½]
---

### 1. ğŸ“Š ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ê°€ ì˜ê²¬
> [ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ í•œì¤„ í‰ê°€]

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì…ë ¥ ë°ì´í„° ê¸¸ì´** | 5ë…„ (Prophet Features) |
| **ë§ˆì§€ë§‰ ê´€ì¸¡ê°’** | [Last Observed Value] |
| **ì‹œê³„ì—´ ì˜ˆì¸¡ê°’** | [Forecast Value] |
| **ì‹ ë¢°ë„** | [Confidence Score] % |

- **ì¶”ì„¸ ë¶„ì„**
  - **ìµœê·¼ ê¸°ê°„ í‰ê· ** (7ì¼) : [Recent Mean]
  - **ì „ ê¸°ê°„ í‰ê· ** : [All-time Mean]
  - **ìµœê·¼ ë³€ë™ ì¶”ì´** : [Trend Analysis: Rising/Falling ë“± ì„¤ëª…]
  - **ì‹œê³„ì—´ ì˜ˆì¸¡ê°’ í•´ì„** : [Forecast Direction] ë°©í–¥ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë©°, ì‹ ë¢°ë„ëŠ” [Confidence Score]% ì…ë‹ˆë‹¤.

- **ì˜ˆì¸¡ê°’ í•´ì„**
  - **í˜„ì¬ ìˆ˜ì¤€ ëŒ€ë¹„** : [Last Value] ëŒ€ë¹„ [Forecast Value] ë¡œ ë³€ë™ ì˜ˆìƒ.
  - **ë‹¨ê¸° ë³€ë™ì„± í‰ê°€** : ë³€ë™ì„± ì§€í‘œ [Volatility Index] ìˆ˜ì¤€.

---

### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„
> [ë‰´ìŠ¤ ê¸°ì‚¬ ê°ì„±ë¶„ì„ í•œì¤„ í‰ê°€]

| ê¸°ì‚¬ ë²ˆí˜¸ | ì œëª© | ì˜í–¥ë ¥ ì ìˆ˜ | ìš”ì•½ |
|-----------|------|-------------|------|
| 1 | [ê¸°ì‚¬ ì œëª©] | [ì ìˆ˜] | [ë‚´ìš© ìš”ì•½] |
| 2 | [ê¸°ì‚¬ ì œëª©] | [ì ìˆ˜] | [ë‚´ìš© ìš”ì•½] |
| ... | ... | ... | ... |

- **ì‹œì¥ ì˜í–¥ë ¥ ë¶„ì„**
  - **ìƒìŠ¹ í™•ë¥ **: [Probability] %
  - **ì¢…í•© ì˜ê²¬**: [ë‰´ìŠ¤ ê¸°ë°˜ ìƒìŠ¹/í•˜ë½ ì˜ˆì¸¡ ì˜ê²¬]

- **í…ìŠ¤íŠ¸ì  ê·¼ê±°**
  - [ê° ê¸°ì‚¬ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„]
  - [ì£¼ìš” í‚¤ì›Œë“œ ë° ê´€ê³„ ì •ë³´(Triple) í™œìš©]

---

### 3. ë¯¸ë˜ ì‹œì¥ ì „ë§

| êµ¬ë¶„ | ê·¼ê±° | ì „ë§ |
|------|------|------|
| **ë‹¨ê¸°(1â€“3ì¼)** | [ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼ ë° ë‰´ìŠ¤ ë‹¨ê¸° ì˜í–¥] | **[ì „ë§]** [ìƒì„¸ ì„¤ëª…] |
| **ì¤‘ê¸°(1ì£¼)** | [ë‰´ìŠ¤ íŠ¸ë Œë“œ ë° ì¤‘ê¸° ì´ìŠˆ] | **[ì „ë§]** [ìƒì„¸ ì„¤ëª…] |
| **ì¥ê¸°(1ê°œì›”)** | [ê±°ì‹œ ê²½ì œ ë° ì •ì±… ë‰´ìŠ¤] | **[ì „ë§]** [ìƒì„¸ ì„¤ëª…] |

- **ìœ„í—˜ ìš”ì¸**
  - [ì£¼ìš” ìœ„í—˜ ìš”ì¸ ë‚˜ì—´]

- **ê¸°íšŒ ìš”ì¸**
  - [ì£¼ìš” ê¸°íšŒ ìš”ì¸ ë‚˜ì—´]

---

### 4. ì¢…í•© ì˜ê²¬

- **[í˜„ì¬ ì‹œì¥ ìƒí™© ìš”ì•½]**
- **[ì£¼ìš” ì§€í‘œ ë° ë‰´ìŠ¤ ìš”ì•½]**
- **[ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° ì „ë§ ìš”ì•½]**
- **[íˆ¬ìì ì…ì¥ì—ì„œì˜ ì¡°ì–¸]**

**ê²°ë¡ **: [ë‚ ì§œ] ê¸°ì¤€, ì‹œì¥ì€ **[ì „ë§]**ì„ ìœ ì§€í•  ê²ƒìœ¼ë¡œ ì „ë§ë˜ë©°, **[ì£¼ìš” ì„±ì¥ ë™ë ¥]**ì´ ì£¼ìš” ì„±ì¥ ë™ë ¥ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ **[ì£¼ìš” ë¦¬ìŠ¤í¬]**ì— ë”°ë¥¸ ë¦¬ìŠ¤í¬ë¥¼ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¤‘ìš”**:
- ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
- í‘œ í˜•ì‹ì€ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ì„¹ì…˜ ë²ˆí˜¸ì™€ ì œëª©ì€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
- ê° ì„¹ì…˜ì€ "---"ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
- ì–¸ì–´ëŠ” ë°˜ë“œì‹œ ìˆœìˆ˜ í•œêµ­ì–´(í•œê¸€)ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."""

SYSTEM_PROMPT = (
    """ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

**ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬**:
1. timeseries_predictor: ì‹œê³„ì—´ ë°ì´í„° ê¸°ë°˜ ì‹œì¥ ì˜ˆì¸¡
   - target_date: ë¶„ì„í•  ëŒ€ìƒ ë‚ ì§œ (í˜•ì‹: "YYYY-MM-DD")
   - ì„¤ëª…: ì§€ì •ëœ ë‚ ì§œì˜ ê°€ê²© ì¶”ì„¸, ì˜ˆì¸¡ê°’, ì‹ ë¢°ë„ ë“±ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

2. news_sentiment_analyzer: ë‰´ìŠ¤ ê¸°ë°˜ ì‹œì¥ ì˜í–¥ë ¥ ë¶„ì„ ë° ê·¼ê±° ì¶”ì¶œ
   - target_date: ë¶„ì„í•  ëŒ€ìƒ ë‚ ì§œ (í˜•ì‹: "YYYY-MM-DD")
   - ì„¤ëª…: í•´ë‹¹ ë‚ ì§œ ì „í›„ì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œì¥ ìƒìŠ¹/í•˜ë½ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ê³ , ì˜ˆì¸¡ì˜ í•µì‹¬ ê·¼ê±°ê°€ ëœ ì£¼ìš” ë‰´ìŠ¤ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

**ë„êµ¬ ì‚¬ìš© ê·œì¹™**:
- ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ(target_date)ê°€ ì£¼ì–´ì§€ë©´ ë°˜ë“œì‹œ ë‘ ë„êµ¬(`timeseries_predictor`, `news_sentiment_analyzer`)ë¥¼ ëª¨ë‘ í˜¸ì¶œí•˜ì—¬ ë°ì´í„°ë¥¼ í™•ë³´í•˜ì„¸ìš”.
- `news_sentiment_analyzer` ê²°ê³¼ì— í¬í•¨ëœ 'evidence_news'ëŠ” ë³´ê³ ì„œì˜ '### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„' ì„¹ì…˜ì˜ í•µì‹¬ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ê° ë‰´ìŠ¤ì˜ ì œëª©ê³¼ ì‹œì¥ ì˜í–¥ë ¥ ì ìˆ˜(price_impact_score)ë¥¼ ë³´ê³ ì„œ í‘œì— í¬í•¨í•˜ì„¸ìš”.
- ë‘ ë„êµ¬ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë…¼ë¦¬ì ì¸ ê¸ˆìœµ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ì‹œê³„ì—´ ì§€í‘œì™€ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ê°€ ì„œë¡œ ë³´ì™„ë˜ë„ë¡ ì„œìˆ í•˜ì„¸ìš”.

**ë³´ê³ ì„œ ì‘ì„± í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤)**:

"""
    + REPORT_FORMAT
)


# LangChain Tools ì •ì˜
@tool
def timeseries_predictor(target_date: str) -> str:
    """
    íŠ¹ì • ë‚ ì§œì˜ ê¸ˆìœµ ì‹œì¥ ì¶”ì„¸(ìƒìŠ¹/í•˜ë½)ì™€ ê°€ê²©ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

    Args:
        target_date: ë¶„ì„í•  ë‚ ì§œ ë¬¸ìì—´ (í˜•ì‹: "YYYY-MM-DD")

    Returns:
        JSON í˜•ì‹ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë¬¸ìì—´ (ì˜ˆì¸¡ê°’, ë°©í–¥, ì‹ ë¢°ë„, ì¶”ì„¸ ë¶„ì„ ë“± í¬í•¨)
    """
    return predict_market_trend(target_date)


@tool
def news_sentiment_analyzer(target_date: str) -> str:
    """
    íŠ¹ì • ë‚ ì§œì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œì¥ ì˜í–¥ë ¥ì„ ì˜ˆì¸¡í•˜ê³  ì£¼ìš” ê·¼ê±° ë‰´ìŠ¤(ì œëª©, ì˜í–¥ë ¥ ì ìˆ˜, ê´€ê³„ ì •ë³´ ë“±)ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    Args:
        target_date: ë¶„ì„í•  ë‚ ì§œ ë¬¸ìì—´ (í˜•ì‹: "YYYY-MM-DD")

    Returns:
        JSON í˜•ì‹ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë¬¸ìì—´ (ìƒìŠ¹ í™•ë¥ , ê·¼ê±° ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸, í”¼ì²˜ ìš”ì•½ í¬í•¨)
    """
    analyzer = SentimentAnalyzer()
    result = analyzer.predict_market_impact(target_date)
    return json.dumps(result, ensure_ascii=False)


class LLMSummarizer:
    """
    Vertex AIë¥¼ ì‚¬ìš©í•˜ëŠ” LangChain Agent ê¸°ë°˜ í†µí•© ë¶„ì„ê¸°

    ì‹œê³„ì—´ ì˜ˆì¸¡ê³¼ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬
    ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Attributes:
        model_name: LLM ëª¨ë¸ëª…
        project_id: GCP í”„ë¡œì íŠ¸ ID
        location: Vertex AI ë¦¬ì „

    Example:
        >>> summarizer = LLMSummarizer()
        >>> result = summarizer.summarize(target_date="2025-01-31")
        >>> print(result["summary"])
    """

    def __init__(
        self,
        model_name: Optional[str] = None,
        project_id: Optional[str] = None,
        location: Optional[str] = None,
    ):
        """
        LLMSummarizer ì´ˆê¸°í™”

        Args:
            model_name: ìƒì„± ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì˜ GENERATE_MODEL_NAME)
            project_id: GCP í”„ë¡œì íŠ¸ ID (ì—†ìœ¼ë©´ ì„¤ì •/gcloudì—ì„œ ê°€ì ¸ì˜´)
            location: Vertex AI ë¦¬ì „ (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì˜ VERTEX_AI_LOCATION)
        """
        self.model_name = model_name or _config.vertex_ai.model_name
        self.location = location or _config.vertex_ai.location
        self._factory = GCPServiceFactory()

        # í”„ë¡œì íŠ¸ ID ê²°ì • (ì„¤ì • â†’ GCPServiceFactory)
        self.project_id = project_id or _config.vertex_ai.project_id
        if not self.project_id:
            # GCPServiceFactoryë¥¼ í†µí•´ í”„ë¡œì íŠ¸ ID í•´ê²°
            self.project_id, _ = self._factory.get_vertex_ai_credentials()

        self.llm = None
        self.agent = None
        self._initialize()

    def _get_access_token(self) -> str:
        """GCPServiceFactoryë¥¼ í†µí•´ ì¸ì¦ í† í° ê°€ì ¸ì˜¤ê¸°"""
        # TODO í•„ìš”í•˜ë©´ ì˜¤ë¥˜ ìˆ˜ì •
        _, credentials = self._factory.get_vertex_ai_credentials()
        return credentials.token

    def _build_base_url(self) -> str:
        """Vertex AI OpenAI í˜¸í™˜ API base URL ìƒì„±"""
        return (
            f"https://{self.location}-aiplatform.googleapis.com/v1/"
            f"projects/{self.project_id}/locations/{self.location}/endpoints/openapi"
        )

    def _create_llm(self, access_token: str) -> ChatOpenAI:
        """ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return ChatOpenAI(
            model=self.model_name,
            base_url=self._build_base_url(),
            api_key=access_token,
            temperature=_config.vertex_ai.temperature,
            max_tokens=_config.vertex_ai.max_tokens,
            model_kwargs={
                "parallel_tool_calls": False,
            },
        )

    def _initialize(self):
        """LLM ë° Agent ì´ˆê¸°í™”"""
        access_token = self._get_access_token()
        self.llm = self._create_llm(access_token)
        logger.info(f"ChatOpenAI (Vertex AI OpenAI í˜¸í™˜ API) ì‚¬ìš©: {self.model_name}")

        tools = [timeseries_predictor, news_sentiment_analyzer]
        llm_with_tools = self.llm.bind_tools(tools)

        self.agent = create_agent(
            model=llm_with_tools,
            tools=tools,
            system_prompt=SYSTEM_PROMPT,
        )

    def _build_user_input(
        self,
        context: str,
        target_date: str,
    ) -> str:
        """Agentì—ê²Œ ì „ë‹¬í•  ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ ìƒì„±"""
        return f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ë¶„ì„ ë§¥ë½**: {context or "ìµœê·¼ ì‹œì¥ ìƒí™© ë¶„ì„"}
**ë¶„ì„ ê¸°ì¤€ ì¼ì**: {target_date}

- `timeseries_predictor`ì™€ `news_sentiment_analyzer` ë„êµ¬ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ {target_date}ì˜ ì‹œì¥ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
"""

    def _validate_output_format(self, summary: str) -> bool:
        """
        ì¶œë ¥ í˜•ì‹ ê²€ì¦ (ìµœì†Œ ê²€ì¦)

        Returns:
            bool: í˜•ì‹ì´ ì˜¬ë°”ë¥´ë©´ True
        """
        if not summary or len(summary.strip()) < 100:
            return False
        return True

    def _extract_summary_from_result(self, result: dict) -> str:
        """Agent ì‹¤í–‰ ê²°ê³¼ì—ì„œ ìš”ì•½ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        messages = result.get("messages", [])

        # messagesì—ì„œ ë§ˆì§€ë§‰ AIMessageì˜ content ì¶”ì¶œ
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                content = str(msg.content) if msg.content else ""
                content = content.strip().rstrip("\\")

                # JSON í˜•ì‹ì˜ tool call argumentsëŠ” ê±´ë„ˆë›°ê¸°
                if content.startswith("{{") and content.strip().endswith("}}"):
                    try:
                        parsed = json.loads(content)
                        if isinstance(parsed, dict) and any(
                            key in parsed for key in ["texts", "data", "target_date"]
                        ):
                            continue
                    except (json.JSONDecodeError, ValueError):
                        pass

                # GPT-OSS-20B íŠ¹ìˆ˜ í˜•ì‹ ê±´ë„ˆë›°ê¸°
                if content.startswith("<|channel|>") and "<|call|>" in content:
                    if not any(
                        keyword in content
                        for keyword in ["ë³´ê³ ì„œ", "ë¶„ì„", "ì˜ê²¬", "ì „ë§", "ì‹œì¥"]
                    ):
                        continue

                if content and len(content) > 50:
                    return content

        # messagesì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° output í•„ë“œ í™•ì¸
        output = result.get("output") or result.get("final_output")
        if output:
            return str(output).strip().rstrip("\\")

        return str(result).strip().rstrip("\\")

    # TODO ì¬ì‹œë„ ë¡œì§ ê°œì„ 
    def summarize(
        self,
        context: str = "",
        target_date: Optional[str] = None,
        max_retries: int = 2,
    ) -> dict:
        """
        LangChain Agentë¥¼ ì´ìš©í•œ LLM ìš”ì•½ ìƒì„±

        Args:
            context: ë¶„ì„ ë§¥ë½
            target_date: ë¶„ì„ ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD)
            max_retries: ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            dict: ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
                - summary: ìƒì„±ëœ ë³´ê³ ì„œ í…ìŠ¤íŠ¸
                - agent_result: Agent ì‹¤í–‰ ê²°ê³¼
        """
        # ë‚ ì§œ ê¸°ë³¸ê°’ (ì˜¤ëŠ˜)
        if not target_date:
            from datetime import datetime

            target_date = datetime.now().strftime("%Y-%m-%d")

        user_input = self._build_user_input(context=context, target_date=target_date)
        summary = ""
        agent_result = {"messages": []}

        for attempt in range(max_retries + 1):
            # Agent ì‹¤í–‰
            if attempt == 0:
                result = self.agent.invoke(
                    {"messages": [HumanMessage(content=user_input)]}
                )
            else:
                result = self.agent.invoke({"messages": result.get("messages", [])})

            # ê²°ê³¼ ì¶”ì¶œ
            if isinstance(result, dict):
                messages = result.get("messages", [])
                summary = self._extract_summary_from_result(result)
                agent_result = result

                # ë””ë²„ê¹… ë¡œê·¸
                tool_call_count = sum(
                    1
                    for msg in messages
                    if isinstance(msg, AIMessage) and msg.tool_calls
                )
                tool_result_count = sum(
                    1 for msg in messages if hasattr(msg, "name") and msg.name
                )
                logger.debug(
                    f"Messages: {len(messages)}, Tool calls: {tool_call_count}, Results: {tool_result_count}"
                )
            else:
                summary = str(result).strip().rstrip("\\")

            # ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° ëŒ€ì²´ í…ìŠ¤íŠ¸ ì°¾ê¸°
            if not summary or len(summary.strip()) < 50:
                logger.warning(
                    f"ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ê¸¸ì´: {len(summary)}ì)"
                )
                if isinstance(result, dict):
                    for msg in reversed(result.get("messages", [])):
                        if isinstance(msg, AIMessage) and msg.content:
                            content = str(msg.content)
                            if (
                                "<|channel|>" not in content
                                and len(content.strip()) > 50
                            ):
                                summary = content.strip()
                                logger.debug(
                                    f"ëŒ€ì²´ í…ìŠ¤íŠ¸ ë°œê²¬ (ê¸¸ì´: {len(summary)}ì)"
                                )
                                break

            # ì¶œë ¥ í˜•ì‹ ê²€ì¦
            if (
                summary
                and len(summary.strip()) > 50
                and self._validate_output_format(summary)
            ):
                return {"summary": summary, "agent_result": agent_result}

            # ì¬ì‹œë„
            if attempt < max_retries:
                logger.warning(
                    f"ì¶œë ¥ í˜•ì‹ ê²€ì¦ ì‹¤íŒ¨. ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})"
                )
                user_input = f"""{user_input}

**ì¤‘ìš”**: ì´ì „ ì‘ë‹µì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”:

{REPORT_FORMAT}

**íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”**:
1. ì„¹ì…˜ ì œëª©ì´ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤: "### 1. ğŸ“Š ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ê°€ ì˜ê²¬", "### 2. ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ê²°ê³¼ ë¶„ì„", "### 3. ë¯¸ë˜ ì‹œì¥ ì „ë§", "### 4. ì¢…í•© ì˜ê²¬"
2. ê° ì„¹ì…˜ì€ "---"ë¡œ êµ¬ë¶„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤ (ìµœì†Œ 3ê°œ)
3. ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹(|)ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
4. í—¤ë”ì— "ğŸ“… ë¶„ì„ ì¼ì"ì™€ "ğŸ’¬ ì¢…í•© ì˜ê²¬"ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
5. Tool í˜¸ì¶œ í›„ ë°˜ë“œì‹œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤"""
            else:
                logger.warning("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. í˜„ì¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                logger.info(f"ìµœì¢… ìš”ì•½ ê¸¸ì´: {len(summary)}ì")
                if summary:
                    logger.info(f"ìµœì¢… ìš”ì•½ ë‚´ìš©: {summary[:200]}...")
                logger.warning("ê²€ì¦ì„ í†µê³¼í•˜ì§€ ëª»í–ˆì§€ë§Œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")

        return {"summary": summary or "", "agent_result": agent_result}
