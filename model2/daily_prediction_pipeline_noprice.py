"""
ì˜¥ìˆ˜ìˆ˜ ê°€ê²© ì˜ˆì¸¡ ì¼ì¼ íŒŒì´í”„ë¼ì¸
- ë§¤ì¼ ì‹¤í–‰ë˜ì–´ í•´ë‹¹ ë‚ ì§œì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒë‚  ê°€ê²© ì˜ˆì¸¡
- ì•™ìƒë¸” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ì˜ˆì¸¡ ìˆ˜í–‰
- ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ íŒ€ì›ë“¤ê³¼ ê³µìœ 

ì‚¬ìš©ë²•:
    # íŠ¹ì • ë‚ ì§œ ì˜ˆì¸¡
    python daily_prediction_pipeline.py --date 2024-02-03
    
    # ì˜¤ëŠ˜ ë‚ ì§œ ìë™ ì˜ˆì¸¡
    python daily_prediction_pipeline.py
    
    # ì—¬ëŸ¬ ë‚ ì§œ ë°°ì¹˜ ì˜ˆì¸¡
    python daily_prediction_pipeline.py --start_date 2024-02-01 --end_date 2024-02-10
"""

import pandas as pd
import numpy as np
import json
import os
import argparse
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from ensemble_predictor import EnsemblePredictor


# ============================================================================
# ë°ì´í„° ì „ì²˜ë¦¬ (train_models.pyì™€ ë™ì¼)
# ============================================================================

class CornDataPreprocessor:
    """ì˜¥ìˆ˜ìˆ˜ ë‰´ìŠ¤ ë° ê°€ê²© ë°ì´í„° ì „ì²˜ë¦¬"""
    
    def __init__(self, news_path, price_path):
        self.news_df = pd.read_csv(news_path)
        self.price_df = pd.read_csv(price_path)
        
        # ë‚ ì§œ ë³€í™˜
        self.news_df['publish_date'] = pd.to_datetime(self.news_df['publish_date'])
        self.price_df['time'] = pd.to_datetime(self.price_df['time'])
    
    def prepare_single_day_data(self, target_date, lookback_days=7):
        """
        íŠ¹ì • ë‚ ì§œì˜ ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„
        
        Args:
            target_date: ì˜ˆì¸¡ ëŒ€ìƒ ë‚ ì§œ (str or datetime)
            lookback_days: ê³¼ê±° ë©°ì¹ ì˜ ë‰´ìŠ¤ë¥¼ ë³¼ ê²ƒì¸ê°€
        
        Returns:
            dict or None: ì „ì²˜ë¦¬ëœ ë°ì´í„° (ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ None)
        """
        if isinstance(target_date, str):
            target_date = pd.to_datetime(target_date)
        
        # í•´ë‹¹ ë‚ ì§œì˜ ê°€ê²© ì •ë³´
        price_row = self.price_df[self.price_df['time'] == target_date]
        
        if len(price_row) == 0:
            print(f"ê²½ê³ : {target_date.date()}ì˜ ê°€ê²© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        price_row = price_row.iloc[0]
        
        # í•´ë‹¹ ë‚ ì§œ ì´ì „ lookback_days ë™ì•ˆì˜ ë‰´ìŠ¤ ìˆ˜ì§‘
        start_date = target_date - timedelta(days=lookback_days)
        relevant_news = self.news_df[
            (self.news_df['publish_date'] >= start_date) & 
            (self.news_df['publish_date'] < target_date) &
            (self.news_df['filter_status'] == 'T')
        ].copy()
        
        if len(relevant_news) == 0:
            print(f"ê²½ê³ : {target_date.date()}ì˜ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # NaN ì²˜ë¦¬
        relevant_news['positive_score'] = relevant_news['positive_score'].fillna(0)
        relevant_news['negative_score'] = relevant_news['negative_score'].fillna(0)
        relevant_news['neutral_score'] = relevant_news['neutral_score'].fillna(0)
        
        # ë‰´ìŠ¤ ì„ë² ë”© íŒŒì‹±
        article_embeddings = []
        entity_embeddings = []
        triple_embeddings = []
        
        for _, news in relevant_news.iterrows():
            try:
                art_emb = self._parse_embedding(news['article_embedding'])
                if art_emb is not None:
                    article_embeddings.append(art_emb)
                
                if pd.notna(news.get('entity_embedding')):
                    ent_emb = self._parse_embedding(news['entity_embedding'])
                    if ent_emb is not None:
                        entity_embeddings.append(ent_emb)
                
                if pd.notna(news.get('triple_embedding')):
                    tri_emb = self._parse_embedding(news['triple_embedding'])
                    if tri_emb is not None:
                        triple_embeddings.append(tri_emb)
            except:
                continue
        
        if len(article_embeddings) == 0:
            print(f"ê²½ê³ : {target_date.date()}ì˜ ìœ íš¨í•œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # í‰ê·  ì„ë² ë”© ê³„ì‚°
        avg_article_emb = np.mean(article_embeddings, axis=0)
        avg_entity_emb = np.mean(entity_embeddings, axis=0) if entity_embeddings else np.zeros(1024)
        avg_triple_emb = np.mean(triple_embeddings, axis=0) if triple_embeddings else np.zeros(1024)
        
        # ê°ì„± ì ìˆ˜ ì§‘ê³„
        sentiment_features = {
            'avg_price_impact': relevant_news['price_impact_score'].mean(),
            'avg_positive': relevant_news['positive_score'].mean(),
            'avg_negative': relevant_news['negative_score'].mean(),
            'avg_neutral': relevant_news['neutral_score'].mean(),
            'sentiment_std': relevant_news['price_impact_score'].std() if len(relevant_news) > 1 else 0,
            'news_count': len(relevant_news),
            'positive_count': int((relevant_news['sentiment'] == 'positive').sum()),
            'negative_count': int((relevant_news['sentiment'] == 'negative').sum()),
            'neutral_count': int((relevant_news['sentiment'] == 'neutral').sum()),
        }
        
        # ê°€ê²© íŠ¹ì„±
        price_features = {
            'open': float(price_row['open']),
            'high': float(price_row['high']),
            'low': float(price_row['low']),
            'close': float(price_row['close']),
            'volume': int(price_row['Volume']),
            'ema': float(price_row['EMA']),
            'volatility': float((price_row['high'] - price_row['low']) / price_row['close'])
        }
        
        return {
            'date': target_date,
            'article_embedding': avg_article_emb,
            'entity_embedding': avg_entity_emb,
            'triple_embedding': avg_triple_emb,
            'sentiment_features': sentiment_features,
            'price_features': price_features,
            'news_articles': relevant_news[[
                'id', 'title', 'description', 'sentiment', 'price_impact_score',
                'positive_score', 'negative_score', 'neutral_score', 'named_entities'
            ]].to_dict('records')
        }
    
    def _parse_embedding(self, emb_str):
        """ë¬¸ìì—´ë¡œ ì €ì¥ëœ ì„ë² ë”©ì„ numpy arrayë¡œ ë³€í™˜"""
        if pd.isna(emb_str):
            return None
        
        try:
            if isinstance(emb_str, str):
                emb = json.loads(emb_str)
            else:
                emb = emb_str
            return np.array(emb, dtype=np.float32)
        except:
            return None


# ============================================================================
# ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±ê¸°
# ============================================================================

class PredictionReportGenerator:
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ ë³´ê³ ì„œë¡œ ë³€í™˜"""
    
    def __init__(self, predictor):
        self.predictor = predictor
    
    def generate_report(self, processed_data, ensemble_result):
        """
        ì¢…í•© ì˜ˆì¸¡ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            processed_data: ì „ì²˜ë¦¬ëœ ë°ì´í„°
            ensemble_result: ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        
        Returns:
            dict: ìµœì¢… ë³´ê³ ì„œ
        """
        # ì˜ˆì¸¡ ë‚ ì§œ ê³„ì‚° (ë‹¤ìŒë‚  ì˜ˆì¸¡)
        prediction_date = processed_data['date'] + timedelta(days=1)
        if isinstance(prediction_date, pd.Timestamp):
            prediction_date_str = prediction_date.strftime('%Y-%m-%d')
            base_date_str = processed_data['date'].strftime('%Y-%m-%d')
        else:
            prediction_date_str = str(prediction_date)[:10]
            base_date_str = str(processed_data['date'])[:10]
        
        # ì£¼ìš” ë‰´ìŠ¤ ì¶”ì¶œ (price_impact_score ê¸°ì¤€)
        news_articles = processed_data['news_articles']
        sorted_articles = sorted(
            news_articles,
            key=lambda x: abs(x.get('price_impact_score', 0)),
            reverse=True
        )
        
        # ì˜ˆì¸¡ ë°©í–¥ì— ë”°ë¥¸ ì¦ê±°/ë°˜ëŒ€ ì¦ê±° ë¶„ë¥˜
        direction = ensemble_result['direction']
        evidence = []
        counter = []
        
        for article in sorted_articles[:15]:  # ìƒìœ„ 15ê°œë§Œ í™•ì¸
            impact = article.get('price_impact_score', 0)
            
            # ì—”í‹°í‹° íŒŒì‹±
            entities_str = article.get('named_entities', '{}')
            try:
                if isinstance(entities_str, str):
                    entities = json.loads(entities_str)
                else:
                    entities = entities_str
                key_entities = list(entities.keys())[:5] if entities else []
            except:
                key_entities = []
            
            article_info = {
                'article_id': int(article['id']),
                'title': article['title'],
                'description': article.get('description', ''),
                'sentiment': article['sentiment'],
                'impact_score': round(float(impact), 3),
                'positive_score': round(float(article.get('positive_score', 0)), 3),
                'negative_score': round(float(article.get('negative_score', 0)), 3),
                'neutral_score': round(float(article.get('neutral_score', 0)), 3),
                'key_entities': key_entities
            }
            
            # ì˜ˆì¸¡ ë°©í–¥ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨
            if direction == "ìƒìŠ¹" and impact > 0.1:
                evidence.append(article_info)
            elif direction == "í•˜ë½" and impact < -0.1:
                evidence.append(article_info)
            elif direction == "ìœ ì§€" and abs(impact) < 0.1:
                evidence.append(article_info)
            else:
                counter.append(article_info)
            
            # ê°ê° ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ
            if len(evidence) >= 5 and len(counter) >= 3:
                break
        
        # ê°ì„± ìš”ì•½
        sentiment_summary = {
            'total_news_count': processed_data['sentiment_features']['news_count'],
            'avg_price_impact': round(processed_data['sentiment_features']['avg_price_impact'], 3),
            'avg_positive_score': round(processed_data['sentiment_features']['avg_positive'], 3),
            'avg_negative_score': round(processed_data['sentiment_features']['avg_negative'], 3),
            'avg_neutral_score': round(processed_data['sentiment_features']['avg_neutral'], 3),
            'positive_ratio': round(
                processed_data['sentiment_features']['positive_count'] / 
                processed_data['sentiment_features']['news_count'], 3
            ),
            'negative_ratio': round(
                processed_data['sentiment_features']['negative_count'] / 
                processed_data['sentiment_features']['news_count'], 3
            ),
            'neutral_ratio': round(
                processed_data['sentiment_features']['neutral_count'] / 
                processed_data['sentiment_features']['news_count'], 3
            )
        }
        
        # ìƒì„¸í•œ reasoning ìƒì„±
        detailed_reasoning = self._generate_detailed_reasoning(
            direction,
            ensemble_result,
            sentiment_summary,
            evidence
        )
        
        # ìµœì¢… ë³´ê³ ì„œ
        report = {
            'metadata': {
                'base_date': base_date_str,
                'prediction_date': prediction_date_str,
                'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'lookback_days': 7
            },
            'prediction': {
                'direction': direction,
                'confidence': round(ensemble_result['confidence'], 3),
                'agreement_level': ensemble_result['agreement_level'],
                'probabilities': {
                    k: round(v, 3) for k, v in ensemble_result['probabilities'].items()
                }
            },
            'model_consensus': {
                'ensemble_reasoning': ensemble_result['reasoning'],
                'detailed_reasoning': detailed_reasoning,
                'model_details': ensemble_result['model_details']
            },
            'evidence': {
                'supporting_news': evidence[:5],
                'opposing_news': counter[:3]
            },
            'market_analysis': {
                'sentiment_summary': sentiment_summary,
                'price_info': {
                    'current_close': processed_data['price_features']['close'],
                    'current_volume': processed_data['price_features']['volume'],
                    'volatility': round(processed_data['price_features']['volatility'], 3),
                    'ema': processed_data['price_features']['ema']
                }
            }
        }
        
        return report
    
    def _generate_detailed_reasoning(self, direction, ensemble_result, sentiment_summary, evidence):
        """ìƒì„¸í•œ ì˜ˆì¸¡ ê·¼ê±° ìƒì„±"""
        reasons = []
        
        # ê°ì„± ë¶„ì„ ê¸°ë°˜
        if direction == "ìƒìŠ¹":
            if sentiment_summary['positive_ratio'] > 0.5:
                reasons.append(
                    f"ê¸ì • ê¸°ì‚¬ ë¹„ìœ¨ {sentiment_summary['positive_ratio']*100:.1f}%ë¡œ ì‹œì¥ ë‚™ê´€ë¡  ìš°ì„¸"
                )
            if sentiment_summary['avg_price_impact'] > 0.1:
                reasons.append(
                    f"í‰ê·  ê°€ê²© ì˜í–¥ë„ {sentiment_summary['avg_price_impact']:.2f}ë¡œ ê¸ì •ì "
                )
        elif direction == "í•˜ë½":
            if sentiment_summary['negative_ratio'] > 0.5:
                reasons.append(
                    f"ë¶€ì • ê¸°ì‚¬ ë¹„ìœ¨ {sentiment_summary['negative_ratio']*100:.1f}%ë¡œ ì‹œì¥ ë¹„ê´€ë¡  í™•ì‚°"
                )
            if sentiment_summary['avg_price_impact'] < -0.1:
                reasons.append(
                    f"í‰ê·  ê°€ê²© ì˜í–¥ë„ {sentiment_summary['avg_price_impact']:.2f}ë¡œ ë¶€ì •ì "
                )
        else:  # ìœ ì§€
            if sentiment_summary['neutral_ratio'] > 0.4:
                reasons.append(
                    f"ì¤‘ë¦½ ê¸°ì‚¬ ë¹„ìœ¨ {sentiment_summary['neutral_ratio']*100:.1f}%ë¡œ ì‹œì¥ ê´€ë§ì„¸"
                )
        
        # ì£¼ìš” ê¸°ì‚¬ ê¸°ë°˜
        if evidence:
            top_article = evidence[0]
            if abs(top_article['impact_score']) > 0.2:
                reasons.append(
                    f"ì£¼ìš” ì´ìŠˆ (ì˜í–¥ë„ {top_article['impact_score']:.2f}): {top_article['title'][:60]}..."
                )
        
        # ëª¨ë¸ í•©ì˜ ê¸°ë°˜
        if ensemble_result['agreement_level'] == 'high':
            reasons.append("3ê°œ ì•™ìƒë¸” ëª¨ë¸ ê°•í•œ í•©ì˜")
        elif ensemble_result['agreement_level'] == 'medium':
            reasons.append("ëª¨ë¸ ê°„ ë¶€ë¶„ í•©ì˜, ì¤‘ê°„ ìˆ˜ì¤€ ì‹ ë¢°ë„")
        else:
            reasons.append("ëª¨ë¸ ê°„ ì˜ê²¬ ì°¨ì´ ì¡´ì¬, ì‹ ì¤‘í•œ í•´ì„ í•„ìš”")
        
        if not reasons:
            reasons.append("ì¤‘ë¦½ì  ë‰´ìŠ¤ íë¦„, ì œí•œì  ê°€ê²© ë³€ë™ ì˜ˆìƒ")
        
        return " | ".join(reasons)


# ============================================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================================================

def predict_single_day(predictor, preprocessor, target_date, output_dir='outputs'):
    """
    íŠ¹ì • ë‚ ì§œì˜ ì˜ˆì¸¡ ìˆ˜í–‰
    
    Args:
        predictor: EnsemblePredictor ì¸ìŠ¤í„´ìŠ¤
        preprocessor: CornDataPreprocessor ì¸ìŠ¤í„´ìŠ¤
        target_date: ì˜ˆì¸¡ ëŒ€ìƒ ë‚ ì§œ (str or datetime)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    
    Returns:
        dict: ì˜ˆì¸¡ ë³´ê³ ì„œ (ì„±ê³µ ì‹œ) or None (ì‹¤íŒ¨ ì‹œ)
    """
    print(f"\n{'='*80}")
    print(f"ì˜ˆì¸¡ ì‹œì‘: {target_date}")
    print(f"{'='*80}")
    
    # ë°ì´í„° ì¤€ë¹„
    print("[1/3] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    processed_data = preprocessor.prepare_single_day_data(target_date, lookback_days=7)
    
    if processed_data is None:
        print(f"ì‹¤íŒ¨: {target_date}ì˜ ë°ì´í„°ë¥¼ ì¤€ë¹„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"  âœ“ {processed_data['sentiment_features']['news_count']}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
    print(f"  âœ“ í‰ê·  ê°ì„± ì˜í–¥ë„: {processed_data['sentiment_features']['avg_price_impact']:.3f}")
    
    # íŠ¹ì„± ë²¡í„° ìƒì„±
    features = np.concatenate([
        processed_data['article_embedding'],  # 512
        processed_data['entity_embedding'],   # 1024
        processed_data['triple_embedding'],   # 1024
        np.array([
            processed_data['sentiment_features']['avg_price_impact'],
            processed_data['sentiment_features']['avg_positive'],
            processed_data['sentiment_features']['avg_negative'],
            processed_data['sentiment_features']['avg_neutral'],
            processed_data['sentiment_features']['sentiment_std'] if not np.isnan(processed_data['sentiment_features']['sentiment_std']) else 0,
            processed_data['sentiment_features']['news_count'],
            processed_data['sentiment_features']['positive_count'],
            processed_data['sentiment_features']['negative_count'],
            processed_data['sentiment_features']['neutral_count'],
        ]),  # 9
        # np.array([
        #     processed_data['price_features']['open'],
        #     processed_data['price_features']['high'],
        #     processed_data['price_features']['low'],
        #     processed_data['price_features']['close'],
        #     processed_data['price_features']['volume'],
        #     processed_data['price_features']['ema'],
        #     processed_data['price_features']['volatility']
        # ])  # 7
    ])  # Total: 2569 (price ì œì™¸)
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    print("\n[2/3] ì•™ìƒë¸” ì˜ˆì¸¡ ì¤‘...")
    ensemble_result = predictor.predict_single(features)
    
    print(f"  âœ“ ì˜ˆì¸¡ ë°©í–¥: {ensemble_result['direction']}")
    print(f"  âœ“ ì‹ ë¢°ë„: {ensemble_result['confidence']:.2%}")
    print(f"  âœ“ í•©ì˜ ìˆ˜ì¤€: {ensemble_result['agreement_level']}")
    
    # ë³´ê³ ì„œ ìƒì„±
    print("\n[3/3] ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    report_generator = PredictionReportGenerator(predictor)
    report = report_generator.generate_report(processed_data, ensemble_result)
    
    # íŒŒì¼ ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    
    if isinstance(target_date, str):
        date_str = target_date
    elif isinstance(target_date, pd.Timestamp):
        date_str = target_date.strftime('%Y-%m-%d')
    else:
        date_str = str(target_date)[:10]
    
    output_path = os.path.join(output_dir, f'news_prediction_{date_str}.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ“ ë³´ê³ ì„œ ì €ì¥: {output_path}")
    print(f"\n{'='*80}")
    print("ì˜ˆì¸¡ ì™„ë£Œ!")
    print(f"{'='*80}\n")
    
    return report


def predict_date_range(predictor, preprocessor, start_date, end_date, output_dir='outputs'):
    """
    ë‚ ì§œ ë²”ìœ„ì— ëŒ€í•œ ë°°ì¹˜ ì˜ˆì¸¡
    
    Args:
        predictor: EnsemblePredictor ì¸ìŠ¤í„´ìŠ¤
        preprocessor: CornDataPreprocessor ì¸ìŠ¤í„´ìŠ¤
        start_date: ì‹œì‘ ë‚ ì§œ (str or datetime)
        end_date: ì¢…ë£Œ ë‚ ì§œ (str or datetime)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    
    Returns:
        list: ì˜ˆì¸¡ ë³´ê³ ì„œ ë¦¬ìŠ¤íŠ¸
    """
    if isinstance(start_date, str):
        start_date = pd.to_datetime(start_date)
    if isinstance(end_date, str):
        end_date = pd.to_datetime(end_date)
    
    print(f"\n{'='*80}")
    print(f"ë°°ì¹˜ ì˜ˆì¸¡: {start_date.date()} ~ {end_date.date()}")
    print(f"{'='*80}\n")
    
    # ë‚ ì§œ ë²”ìœ„ ìƒì„±
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    
    results = []
    success_count = 0
    fail_count = 0
    
    for current_date in date_range:
        try:
            report = predict_single_day(predictor, preprocessor, current_date, output_dir)
            if report is not None:
                results.append(report)
                success_count += 1
            else:
                fail_count += 1
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ ({current_date.date()}): {e}")
            fail_count += 1
    
    print(f"\n{'='*80}")
    print(f"ë°°ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ: ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {fail_count}ê±´")
    print(f"{'='*80}\n")
    
    return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì˜¥ìˆ˜ìˆ˜ ê°€ê²© ì¼ì¼ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸')
    
    # ë°ì´í„° ê²½ë¡œ
    parser.add_argument('--news_path', type=str, default='corn_all_news_with_sentiment.csv',
                       help='ë‰´ìŠ¤ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--price_path', type=str, default='corn_future_price.csv',
                       help='ê°€ê²© ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ')
    
    # ëª¨ë¸ ê²½ë¡œ
    parser.add_argument('--model_dir', type=str, default='trained_models',
                       help='í•™ìŠµëœ ëª¨ë¸ ë””ë ‰í† ë¦¬')
    
    # ì˜ˆì¸¡ ë‚ ì§œ
    parser.add_argument('--date', type=str, default=None,
                       help='ì˜ˆì¸¡ ë‚ ì§œ (YYYY-MM-DD), ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ')
    parser.add_argument('--start_date', type=str, default=None,
                       help='ë°°ì¹˜ ì˜ˆì¸¡ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--end_date', type=str, default=None,
                       help='ë°°ì¹˜ ì˜ˆì¸¡ ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)')
    
    # ì¶œë ¥ ê²½ë¡œ
    parser.add_argument('--output_dir', type=str, default='outputs',
                       help='ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("ì˜¥ìˆ˜ìˆ˜ ê°€ê²© ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸")
    print("="*80)
    
    # 1. ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
    print("\n[1/3] ë°ì´í„° ë¡œë” ì´ˆê¸°í™” ì¤‘...")
    preprocessor = CornDataPreprocessor(
        news_path=args.news_path,
        price_path=args.price_path
    )
    print(f"  âœ“ ë‰´ìŠ¤ ë°ì´í„°: {len(preprocessor.news_df)}ê°œ ê¸°ì‚¬")
    print(f"  âœ“ ê°€ê²© ë°ì´í„°: {len(preprocessor.price_df)}ê°œ ë ˆì½”ë“œ")
    
    # 2. ì•™ìƒë¸” ì˜ˆì¸¡ê¸° ë¡œë“œ
    print(f"\n[2/3] ì•™ìƒë¸” ì˜ˆì¸¡ê¸° ë¡œë“œ ì¤‘... (from {args.model_dir})")
    predictor = EnsemblePredictor(model_dir=args.model_dir)
    
    # 3. ì˜ˆì¸¡ ìˆ˜í–‰
    print(f"\n[3/3] ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    
    if args.start_date and args.end_date:
        # ë°°ì¹˜ ì˜ˆì¸¡
        results = predict_date_range(
            predictor, 
            preprocessor, 
            args.start_date, 
            args.end_date,
            args.output_dir
        )
        
        # ìš”ì•½ ì¶œë ¥
        if results:
            print("\nì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
            for result in results[-5:]:  # ë§ˆì§€ë§‰ 5ê°œë§Œ ì¶œë ¥
                print(f"  - {result['metadata']['prediction_date']}: "
                      f"{result['prediction']['direction']} "
                      f"(ì‹ ë¢°ë„ {result['prediction']['confidence']:.2%})")
    
    else:
        # ë‹¨ì¼ ì˜ˆì¸¡
        if args.date:
            target_date = args.date
        else:
            # ì˜¤ëŠ˜ ë‚ ì§œ (ë˜ëŠ” ë°ì´í„°ì—ì„œ ê°€ì¥ ìµœê·¼ ë‚ ì§œ)
            price_date = preprocessor.price_df['time'].max()
            news_date = preprocessor.news_df['publish_date'].max()
            target_date = min(price_date, news_date)
            print(f"  â„¹ï¸  ë‚ ì§œ ë¯¸ì§€ì •, ìµœê·¼ ë°ì´í„° ì‚¬ìš©: {target_date.date()}")
        
        result = predict_single_day(
            predictor,
            preprocessor,
            target_date,
            args.output_dir
        )
        
        # ê²°ê³¼ ì¶œë ¥
        if result:
            print("\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼:")
            print(f"  ì˜ˆì¸¡ ë‚ ì§œ: {result['metadata']['prediction_date']}")
            print(f"  ì˜ˆì¸¡ ë°©í–¥: {result['prediction']['direction']}")
            print(f"  ì‹ ë¢°ë„: {result['prediction']['confidence']:.2%}")
            print(f"  í•©ì˜ ìˆ˜ì¤€: {result['prediction']['agreement_level']}")
            print(f"\n  í™•ë¥  ë¶„í¬:")
            for k, v in result['prediction']['probabilities'].items():
                print(f"    {k}: {v:.2%}")
            print(f"\n  ì˜ˆì¸¡ ê·¼ê±°:")
            print(f"    {result['model_consensus']['detailed_reasoning']}")
            
            print(f"\n  ë‰´ìŠ¤ ë¶„ì„:")
            print(f"    ì´ ë‰´ìŠ¤ ìˆ˜: {result['market_analysis']['sentiment_summary']['total_news_count']}")
            print(f"    ê¸ì • ë¹„ìœ¨: {result['market_analysis']['sentiment_summary']['positive_ratio']:.2%}")
            print(f"    ë¶€ì • ë¹„ìœ¨: {result['market_analysis']['sentiment_summary']['negative_ratio']:.2%}")
            print(f"    ì¤‘ë¦½ ë¹„ìœ¨: {result['market_analysis']['sentiment_summary']['neutral_ratio']:.2%}")
    
    print("\n" + "="*80)
    print("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
